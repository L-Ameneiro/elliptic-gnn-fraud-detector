{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b8d0c6",
   "metadata": {},
   "source": [
    "## **Practico 2: Entrenamos modelos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3b5dc",
   "metadata": {},
   "source": [
    "1) A partir del grafo de elliptic dataset provisto en la librer√≠a torch-geometric, generar una versi√≥n de tabla csv, a partir de la cual se podr√≠a haber construido el mismo. Comprender en profundidad como se construye un grafo a partir de los datos de las transacciones y viceversa.\n",
    "\n",
    "2) (Tarea mas importante) Tomar como punto de partida el elliptic dataset provisto en la librer√≠a torch-geometric y comenzar una prueba de concepto buscando entrenar un modelo de machine learning capaz de predecir el fraude. Pueden trabajar con el archivo en formato grafo o csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9753d",
   "metadata": {},
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ba1f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import torch_geometric\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1793466",
   "metadata": {},
   "source": [
    "## 1- Generar CSV a partir de un grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4316657",
   "metadata": {},
   "source": [
    "Se comienza por importar el grafo de la librer√≠a torch-geometric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd4d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "\n",
    "dataset = EllipticBitcoinDataset(root='data/elliptic')\n",
    "data = dataset[0]  # Es un solo grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2175d7c",
   "metadata": {},
   "source": [
    "Se hecha un vistazo a los principales atributos de dicho grafo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcec0e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atributos disponibles en el grafo:\n",
      "['edge_index', 'x', 'test_mask', 'y', 'train_mask']\n"
     ]
    }
   ],
   "source": [
    "# Mostrar todos los atributos disponibles\n",
    "print(\"Atributos disponibles en el grafo:\")\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615155ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de features (x): torch.Size([203769, 165])\n",
      "Aristas (edge_index): torch.Size([2, 234355])\n",
      "Etiquetas (y): torch.Size([203769])\n"
     ]
    }
   ],
   "source": [
    "# Ver tama√±os individuales\n",
    "print(\"Matriz de features (x):\", data.x.shape)\n",
    "print(\"Aristas (edge_index):\", data.edge_index.shape)\n",
    "print(\"Etiquetas (y):\", data.y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08d47ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodos: 203769\n",
      "Aristas: 234355\n"
     ]
    }
   ],
   "source": [
    "# Otras caracter√≠sticas\n",
    "print(\"Nodos:\", data.num_nodes)\n",
    "print(\"Aristas:\", data.num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88867a3",
   "metadata": {},
   "source": [
    "El grafo provisto por la librer√≠a `torch_geometric` contiene toda la informaci√≥n necesaria para su construcci√≥n. Desde el c√≥digo, podemos explorar f√°cilmente los atributos del objeto `data` (el grado en s√≠):\n",
    "\n",
    "- `data.x`: matriz de caracter√≠sticas de cada nodo, con 166 features por transacci√≥n\n",
    "- `data.y`: etiquetas de cada nodo (0 = leg√≠timo, 1 = fraude, 2 = desconocido)\n",
    "- `data.edge_index`: matriz con las conexiones entre nodos (aristas del grafo)\n",
    "- `data.num_nodes`: cantidad total de nodos (transacciones)\n",
    "- `data.num_edges`: cantidad total de aristas (conexiones entre transacciones)\n",
    "\n",
    "Como puede verse, estos datos est√°n organizados en estructuras separadas, y no es posible representar **toda esta informaci√≥n** de forma fiel en un solo archivo `.csv` plano sin perder relaciones o duplicar contenido innecesariamente.\n",
    "\n",
    "Por eso, se opta por generar dos archivos `.csv`:\n",
    "1. `nodes.csv`: contiene una fila por transacci√≥n, con su ID, etiqueta y features.\n",
    "2. `edges.csv`: contiene una fila por relaci√≥n, indicando c√≥mo se conectan dos transacciones (nodos).\n",
    "\n",
    "Esta separaci√≥n permite reconstruir el grafo completo y representa correctamente la naturaleza del dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9c8e5",
   "metadata": {},
   "source": [
    "Generar `nodes.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5a70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_155</th>\n",
       "      <th>feature_156</th>\n",
       "      <th>feature_157</th>\n",
       "      <th>feature_158</th>\n",
       "      <th>feature_159</th>\n",
       "      <th>feature_160</th>\n",
       "      <th>feature_161</th>\n",
       "      <th>feature_162</th>\n",
       "      <th>feature_163</th>\n",
       "      <th>feature_164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.171469</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562153</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947382</td>\n",
       "      <td>0.673103</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670883</td>\n",
       "      <td>0.439728</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782743</td>\n",
       "      <td>12.414557</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511871</td>\n",
       "      <td>-0.400422</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   node_id  fraud_label  feature_0  feature_1  feature_2  feature_3  \\\n",
       "0        0            2  -0.171469  -0.184668  -1.201369  -0.121970   \n",
       "1        1            2  -0.171484  -0.184668  -1.201369  -0.121970   \n",
       "2        2            2  -0.172107  -0.184668  -1.201369  -0.121970   \n",
       "3        3            0   0.163054   1.963790  -0.646376  12.409294   \n",
       "4        4            2   1.011523  -0.081127  -1.201369   1.153668   \n",
       "\n",
       "   feature_4  feature_5  feature_6  feature_7  ...  feature_155  feature_156  \\\n",
       "0  -0.043875  -0.113002  -0.061584  -0.162097  ...    -0.562153    -0.600999   \n",
       "1  -0.043875  -0.113002  -0.061584  -0.162112  ...     0.947382     0.673103   \n",
       "2  -0.043875  -0.113002  -0.061584  -0.162749  ...     0.670883     0.439728   \n",
       "3  -0.063725   9.782743  12.414557  -0.163645  ...    -0.577099    -0.613614   \n",
       "4   0.333276   1.312656  -0.061584  -0.163523  ...    -0.511871    -0.400422   \n",
       "\n",
       "   feature_157  feature_158  feature_159  feature_160  feature_161  \\\n",
       "0     1.461330     1.461369     0.018279    -0.087490    -0.131155   \n",
       "1    -0.979074    -0.978556     0.018279    -0.087490    -0.131155   \n",
       "2    -0.979074    -0.978556    -0.098889    -0.106715    -0.131155   \n",
       "3     0.241128     0.241406     1.072793     0.085530    -0.131155   \n",
       "4     0.517257     0.579382     0.018279     0.277775     0.326394   \n",
       "\n",
       "   feature_162  feature_163  feature_164  \n",
       "0    -0.097524    -0.120613    -0.119792  \n",
       "1    -0.097524    -0.120613    -0.119792  \n",
       "2    -0.183671    -0.120613    -0.119792  \n",
       "3     0.677799    -0.120613    -0.119792  \n",
       "4     1.293750     0.178136     0.179117  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Features por transacci√≥n (nodo)\n",
    "features = data.x.numpy()  # shape: (203769, 166)\n",
    "\n",
    "# Etiquetas: 0 = leg√≠timo, 1 = fraude, 2 = desconocido\n",
    "labels = data.y.numpy()  # shape: (203769,)\n",
    "\n",
    "# Construir DataFrame\n",
    "df_nodes = pd.DataFrame(features, columns=[f'feature_{i}' for i in range(features.shape[1])])\n",
    "df_nodes.insert(0, 'fraud_label', labels)\n",
    "df_nodes.insert(0, 'node_id', range(data.num_nodes))\n",
    "\n",
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc06360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar\n",
    "df_nodes.to_csv(\"nodes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9973c3",
   "metadata": {},
   "source": [
    "Generar `edges.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e881244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from_node</th>\n",
       "      <th>to_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from_node  to_node\n",
       "0          0        1\n",
       "1          2        3\n",
       "2          4        5\n",
       "3          6        7\n",
       "4          8        9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aristas (relaciones entre transacciones)\n",
    "edge_index = data.edge_index.numpy()  # shape: (2, 234355)\n",
    "\n",
    "df_edges = pd.DataFrame({\n",
    "    'from_node': edge_index[0],\n",
    "    'to_node': edge_index[1]\n",
    "})\n",
    "\n",
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5ed9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar\n",
    "df_edges.to_csv(\"edges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb923cb8",
   "metadata": {},
   "source": [
    "Ambos `csv` generados ser√°n suficientes para realizar el camino inverso y lograr construir un grafo de transacciones a partir de ellos, dando a entender que tanto una tabla como un grafo son 2 maneras de representar lo mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb2c7c",
   "metadata": {},
   "source": [
    "### Descripci√≥n del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e3d55",
   "metadata": {},
   "source": [
    "Continuaremos trabajando con Elliptic Data Set, presentado por Weber et al. en 2019 [1], el cual contiene datos anonimizados de 203,769 transacciones de Bitcoin. De la documentaci√≥n del dataset sabemos que:\n",
    "\n",
    "2% de las transacciones est√°n clasificadas como il√≠citas (clase 1), 21% como l√≠citas (clase 2) y el 77% restante no ha sido etiquetado (clase 0). \n",
    "\n",
    "#### Features \n",
    "Cada transacci√≥n corresponde a un intervalo temporal de 3 hs (time step). Los time step son √≠ndices del 1 al 49 y est√°n espaciados entre s√≠ por dos semanas. Cada time step contiene un componente de transacciones conectadas entre s√≠, y desconectadas del resto de los componentes. \n",
    "\n",
    "De cada transacci√≥n se incluyen tambi√©n 166 variables (features) cuya descripci√≥n exacta no es provista, pero se dan rasgos generales:\n",
    "\n",
    "##### Features directas de la transacci√≥n (1 a 94)\n",
    "Las primeras 94 features representan informaci√≥n directa de la transacci√≥n. Esto incluye al time-step, la cantidad de inputs/outputs, la tarifa de transacci√≥n, el volumen de output, as√≠ como variables agregadas tales como la cantidad promedio de BTC recibido (gastado) por los inputs/outputs y el n√∫mero promedio de transacciones entrantes asociadas con los inputs/outpus.\n",
    "\n",
    "Estas son estad√≠sticas directamente relacionadas con:\n",
    "- C√≥mo se construy√≥ la transacci√≥n\n",
    "- De qu√© magnitud es\n",
    "- Qu√© tan activa es la wallet involucrada\n",
    "- Cu√°nto dinero se movi√≥\n",
    "\n",
    "##### Features agregadas de transacciones vecinas (95 a 166)\n",
    "Las 72 features restantes provee estad√≠sticas agregadas de las transacciones vecinas (una arista de distancia). Para cada variable incluida en las features directas, se calculan agregados (m√°ximo, m√≠nimo, desv√≠o est√°ndar y coeficientes de correlaci√≥n) de las transacciones vecinas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86806187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los archivos\n",
    "features = pd.read_csv(r\"D:\\Documents\\Cursos\\Diplomatura FAMAF\\Mentoria\\elliptic-gnn-fraud-detector\\data\\elliptic\\elliptic_txs_features.csv\", header=None)\n",
    "edges = pd.read_csv(r\"D:\\Documents\\Cursos\\Diplomatura FAMAF\\Mentoria\\elliptic-gnn-fraud-detector\\data\\elliptic\\elliptic_txs_edgelist.csv\")\n",
    "classes = pd.read_csv(r\"D:\\Documents\\Cursos\\Diplomatura FAMAF\\Mentoria\\elliptic-gnn-fraud-detector\\data\\elliptic\\elliptic_txs_classes.csv\")\n",
    "\n",
    "# Renombrar columnas para claridad\n",
    "# features = features.rename(columns={0: \"txId\", 1: \"time_step\"})\n",
    "edges = edges.rename(columns={\"txId1\": \"source\", \"txId2\": \"target\"})\n",
    "classes = classes.rename(columns={\"txId\": \"txId\", \"class\": \"label\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4309caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renombrar columnas para mayor claridad\n",
    "tx_features = [\"tx_feat_\"+str(i) for i in range(2,95)]\n",
    "agg_features = [\"agg_feat_\"+str(i) for i in range(1,73)]\n",
    "features.columns = [\"txId\",\"time_step\"] + tx_features + agg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95950b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "    Features :  203,769 (rows)   167 (cols)\n",
      "    Classes  :  203,769 (rows)     2 (cols)\n",
      "    Edgelist :  234,355 (rows)     2 (cols)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Shapes\n",
    "{4*' '}Features : {features.shape[0]:8,} (rows)  {features.shape[1]:4,} (cols)\n",
    "{4*' '}Classes  : {classes.shape[0]:8,} (rows)  {classes.shape[1]:4,} (cols)\n",
    "{4*' '}Edgelist : {edges.shape[0]:8,} (rows)  {edges.shape[1]:4,} (cols)\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa83d4",
   "metadata": {},
   "source": [
    "#### Armado de dataset para modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acd68c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with classes\n",
    "df_features = pd.merge(features, classes, left_on='txId', right_on='txId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bc7f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['label'] = df_features['label'].apply(lambda x: '0' if x == \"unknown\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "883891df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    157205\n",
       "1      4545\n",
       "2     42019\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chequeamos que las clases nos quedaron bien armadas\n",
    "df_features.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cba84",
   "metadata": {},
   "source": [
    "## 2 - Entrenamiento de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd282713",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbe7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0),\n",
       " txId           0\n",
       " time_step      0\n",
       " tx_feat_2      0\n",
       " tx_feat_3      0\n",
       " tx_feat_4      0\n",
       "               ..\n",
       " agg_feat_69    0\n",
       " agg_feat_70    0\n",
       " agg_feat_71    0\n",
       " agg_feat_72    0\n",
       " label          0\n",
       " Length: 168, dtype: int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de filas con al menos un valor nulo\n",
    "num_rows_with_nan = df_features.isnull().any(axis=1).sum()\n",
    "\n",
    "# Cantidad de datos faltantes por variable\n",
    "missing_per_col = df_features.isnull().sum()\n",
    "\n",
    "(num_rows_with_nan, missing_per_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a42a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nos quedamos con datos etiquetados\n",
    "data = df_features[(df_features['label']=='1') | (df_features['label']=='2')]\n",
    "\n",
    "# creamos series para modelado\n",
    "X = data[['time_step'] + tx_features+agg_features]\n",
    "y = data['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56dfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divisi√≥n entre entrenamiento y evaluaci√≥n\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a4f4b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuci√≥n original: label\n",
      "2    0.902392\n",
      "1    0.097608\n",
      "Name: proportion, dtype: float64\n",
      "Train: label\n",
      "2    0.902123\n",
      "1    0.097877\n",
      "Name: proportion, dtype: float64\n",
      "Test: label\n",
      "2    0.903468\n",
      "1    0.096532\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distribuci√≥n de variable target en subconjuntos de train y test\n",
    "print(\"Distribuci√≥n original:\", y.value_counts(normalize=True))\n",
    "print(\"Train:\", y_train.value_counts(normalize=True))\n",
    "print(\"Test:\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21bd6b",
   "metadata": {},
   "source": [
    "Se observa una correcta distirbuci√≥n entre los subconjuntos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b58348",
   "metadata": {},
   "source": [
    "### Baseline model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97d340",
   "metadata": {},
   "source": [
    "Se propone establecer un baseline score con un modelo simple, y luego intentamos superarlo (‚Äòbreak the baseline‚Äô) con modelos m√°s sofisticados.\n",
    "\n",
    "Para ello se utilizar√° DummyClassifier de sklearn con (strategy=\"stratified\") que predice al azar, pero manteniendo las proporciones de clases del conjunto de entrenamiento. Esto √∫ltimo es importante ya que se tiene un conjunto de datos fuertemente desbalanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1514c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Crear el modelo baseline\n",
    "dummy = DummyClassifier(strategy='stratified', random_state=10)  # estrategia: predecir al azar manteniendo las proporciones\n",
    "\n",
    "# 2. Entrenarlo con el conjunto de entrenamiento\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predecir sobre el conjunto de test\n",
    "y_pred = dummy.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1172d1b",
   "metadata": {},
   "source": [
    "Se defininen m√©tricas para evaluar el modelo (y los siguientes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e13b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Resultados usando DummyClassifier (baseline)\n",
      "Accuracy: 0.8244389562976484\n",
      "Precision: 0.083710407239819\n",
      "Recall: 0.08231368186874305\n",
      "F1 score: 0.08300616937745373\n",
      "Matriz de confusi√≥n:\n",
      " [[  74  825]\n",
      " [ 810 7604]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Resultados usando DummyClassifier (baseline)\")\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",    recall_score(y_test, y_pred))\n",
    "print(\"F1 score:\",  f1_score(y_test, y_pred))\n",
    "print(\"Matriz de confusi√≥n:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119d685",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n\n",
    "Si bien los resultados presentan una buena accuracy, ese valor es enga√±oso debido al desbalance del dataset.\n",
    "\n",
    "De ahi en mas, todas las m√©tricas son muy bajas incluyendo un F1 pobre, lo cual es correcto, ya que ahora se tiene un baseline para romper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1dcf1",
   "metadata": {},
   "source": [
    "## Modelos lineales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cfed0",
   "metadata": {},
   "source": [
    "### Evaluando el poder explicativo de tx_features y agg_features\n",
    "\n",
    "En este conjuntos de datos se cuenta con dos grupos de features: uno que corresponde a caracter√≠sticas de cada transaccion y otro que corresponde a caracter√≠sticas de su contexto inmediato. \n",
    "Para evaluar el valor explicativo de cada grupo de features, entrenamos tres tipos de modelos:\n",
    "- dos modelos parciales: s√≥lo tx_features (incluye time-step) y s√≥lo agg_features. \n",
    "- un modelo completo (all_features)\n",
    "\n",
    "Realizamos este an√°lisis tanto con modelos de clasificaci√≥n por regresi√≥n log√≠stica como por descenso de gradiente.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44992e",
   "metadata": {},
   "source": [
    "#### Regresi√≥n log√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47978afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Resultados usando solo features de transacci√≥n\n",
      "Accuracy: 0.9076592698639943\n",
      "Precision: 0.6703296703296703\n",
      "Recall: 0.13232104121475055\n",
      "F1 score: 0.2210144927536232\n",
      "Matriz de confusi√≥n:\n",
      " [[  183  1200]\n",
      " [   90 12497]]\n",
      "\n",
      "üîπ Resultados usando solo features agregadas\n",
      "Accuracy: 0.916034359341446\n",
      "Precision: 0.7536231884057971\n",
      "Recall: 0.22559652928416485\n",
      "F1 score: 0.34724540901502504\n",
      "Matriz de confusi√≥n:\n",
      " [[  312  1071]\n",
      " [  102 12485]]\n",
      "\n",
      "üîπ Resultados usando todas las features\n",
      "Accuracy: 0.9584108804581245\n",
      "Precision: 0.8369747899159664\n",
      "Recall: 0.720173535791757\n",
      "F1 score: 0.7741935483870968\n",
      "Matriz de confusi√≥n:\n",
      " [[  996   387]\n",
      " [  194 12393]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = data['label'].astype(int)\n",
    "\n",
    "# Divisi√≥n en train/test\n",
    "X_tx = data[tx_features]\n",
    "X_agg = data[agg_features]\n",
    "X_all = data[tx_features + agg_features]\n",
    "\n",
    "X_train_tx, X_test_tx, y_train, y_test = train_test_split(X_tx, y, test_size=0.3, random_state=10)\n",
    "X_train_agg, X_test_agg = train_test_split(X_agg, test_size=0.3, random_state=10)\n",
    "X_train_all, X_test_all = train_test_split(X_all, test_size=0.3, random_state=10)\n",
    "\n",
    "# Escalado (opcional pero recomendado)\n",
    "scaler_tx = StandardScaler().fit(X_train_tx)\n",
    "scaler_agg = StandardScaler().fit(X_train_agg)\n",
    "scaler_all = StandardScaler().fit(X_train_all)\n",
    "\n",
    "X_train_tx = scaler_tx.transform(X_train_tx)\n",
    "X_test_tx = scaler_tx.transform(X_test_tx)\n",
    "\n",
    "X_train_agg = scaler_agg.transform(X_train_agg)\n",
    "X_test_agg = scaler_agg.transform(X_test_agg)\n",
    "\n",
    "X_train_all = scaler_all.transform(X_train_all)\n",
    "X_test_all = scaler_all.transform(X_test_all)\n",
    "\n",
    "# Entrenar modelos\n",
    "def eval_model(X_train, y_train, X_test, y_test, name):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\nüîπ Resultados usando {name}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Matriz de confusi√≥n:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Evaluar cada grupo de features\n",
    "eval_model(X_train_tx, y_train, X_test_tx, y_test, \"solo features de transacci√≥n\")\n",
    "eval_model(X_train_agg, y_train, X_test_agg, y_test, \"solo features agregadas\")\n",
    "eval_model(X_train_all, y_train, X_test_all, y_test, \"todas las features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e4e23",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n\n",
    "Los resultados muestran que, al estimar una clasificaci√≥n por regresi√≥n lineal, tanto las caracter√≠sticas de la transacci√≥n como de su contexto inmediato tienen bajo valor predictivo, con F1 scores de 0.22 y 0.35 respectivamente. \n",
    "\n",
    "Ambos modelos parciales muestran muy baja recall, indicando que no est√°n pudiendo detectar gran parte de los casos etiquetados como fraude. \n",
    "\n",
    "El modelo con todas las features tiene un mejor desempe√±o, evidenciado en su mayor score F1, de 0.77, que se debe sobre todo a su mejor valor de recall al reducir los falsos negativos (clave en este tipo de problemas de detecci√≥n de fraude). Esto indica que la combinaci√≥n de las features de la transacci√≥n y de su contexto son necesarias para predecir m√°s eficazmente los casos de fraude. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d12c037",
   "metadata": {},
   "source": [
    "#### Clasificador por gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffca1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Resultados usando solo features de transacci√≥n (SGDClassifier)\n",
      "Accuracy: 0.9087329992841804\n",
      "Precision: 0.5973741794310722\n",
      "Recall: 0.2001466275659824\n",
      "F1 score: 0.29983525535420097\n",
      "Matriz de confusi√≥n:\n",
      " [[  273  1091]\n",
      " [  184 12422]]\n",
      "\n",
      "üîπ Resultados usando solo features agregadas (SGDClassifier)\n",
      "Accuracy: 0.9234073013600572\n",
      "Precision: 0.7370967741935484\n",
      "Recall: 0.3350439882697947\n",
      "F1 score: 0.46068548387096775\n",
      "Matriz de confusi√≥n:\n",
      " [[  457   907]\n",
      " [  163 12443]]\n",
      "\n",
      "üîπ Resultados usando todas las features (SGDClassifier)\n",
      "Accuracy: 0.9592698639942735\n",
      "Precision: 0.8545941123996432\n",
      "Recall: 0.7023460410557185\n",
      "F1 score: 0.7710261569416499\n",
      "Matriz de confusi√≥n:\n",
      " [[  958   406]\n",
      " [  163 12443]]\n"
     ]
    }
   ],
   "source": [
    "y = data['label'].astype(int)\n",
    "\n",
    "# Divisiones con estratificaci√≥n\n",
    "X_tx = data[tx_features]\n",
    "X_agg = data[agg_features]\n",
    "X_all = data[tx_features + agg_features]\n",
    "\n",
    "X_train_tx, X_test_tx, y_train, y_test = train_test_split(\n",
    "    X_tx, y, test_size=0.3, random_state=10, stratify=y\n",
    ")\n",
    "X_train_agg, X_test_agg = train_test_split(\n",
    "    X_agg, test_size=0.3, random_state=10, stratify=y\n",
    ")\n",
    "X_train_all, X_test_all = train_test_split(\n",
    "    X_all, test_size=0.3, random_state=10, stratify=y\n",
    ")\n",
    "\n",
    "# Escalado\n",
    "scaler_tx = StandardScaler().fit(X_train_tx)\n",
    "scaler_agg = StandardScaler().fit(X_train_agg)\n",
    "scaler_all = StandardScaler().fit(X_train_all)\n",
    "\n",
    "X_train_tx = scaler_tx.transform(X_train_tx)\n",
    "X_test_tx = scaler_tx.transform(X_test_tx)\n",
    "\n",
    "X_train_agg = scaler_agg.transform(X_train_agg)\n",
    "X_test_agg = scaler_agg.transform(X_test_agg)\n",
    "\n",
    "X_train_all = scaler_all.transform(X_train_all)\n",
    "X_test_all = scaler_all.transform(X_test_all)\n",
    "\n",
    "# Evaluaci√≥n con SGDClassifier\n",
    "def eval_sgd(X_train, y_train, X_test, y_test, name):\n",
    "    clf = SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\nüîπ Resultados usando {name} (SGDClassifier)\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Matriz de confusi√≥n:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Ejecutar evaluaciones\n",
    "eval_sgd(X_train_tx, y_train, X_test_tx, y_test, \"solo features de transacci√≥n\")\n",
    "eval_sgd(X_train_agg, y_train, X_test_agg, y_test, \"solo features agregadas\")\n",
    "eval_sgd(X_train_all, y_train, X_test_all, y_test, \"todas las features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7889d2",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n\n",
    "Nuevamente verificamos que la combinaci√≥n de features de la transacci√≥n y de su contexto es lo que mejor permite predecir los casos de fraude: se logra un buen nivel de recall, logrando predecir correctamente el 70% de los casos de fraude. Al mismo tiempo, se mantiene un buen nivel de precisi√≥n, lo que indica baja proporci√≥n de falsos positivos. \n",
    "\n",
    "Sin embargo, vemos que en este modelo ambos modelos parciales difieren notablemente en su capacidad predictiva, siendo las features del contexto las que explican m√°s varianza y tienen mayor poder predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4eaccb",
   "metadata": {},
   "source": [
    "#### Buscando el mejor modelo por descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "536440dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©tricas funci√≥n\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b223fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    classifier__alpha  classifier__eta0 classifier__learning_rate  \\\n",
       " 4              0.0001             0.001                   optimal   \n",
       " 13             0.0001             0.010                   optimal   \n",
       " 22             0.0001             0.100                   optimal   \n",
       " 3              0.0001             0.001                   optimal   \n",
       " 12             0.0001             0.010                   optimal   \n",
       " 21             0.0001             0.100                   optimal   \n",
       " 23             0.0001             0.100                   optimal   \n",
       " 14             0.0001             0.010                   optimal   \n",
       " 5              0.0001             0.001                   optimal   \n",
       " 31             0.0010             0.001                   optimal   \n",
       " \n",
       "    classifier__loss   mean_f1    std_f1  \n",
       " 4             hinge  0.781892  0.038922  \n",
       " 13            hinge  0.781892  0.038922  \n",
       " 22            hinge  0.781892  0.038922  \n",
       " 3          log_loss  0.758008  0.006926  \n",
       " 12         log_loss  0.758008  0.006926  \n",
       " 21         log_loss  0.758008  0.006926  \n",
       " 23   modified_huber  0.747200  0.034520  \n",
       " 14   modified_huber  0.747200  0.034520  \n",
       " 5    modified_huber  0.747200  0.034520  \n",
       " 31            hinge  0.742883  0.005001  ,\n",
       " {'classifier__alpha': 0.0001,\n",
       "  'classifier__eta0': 0.001,\n",
       "  'classifier__learning_rate': 'optimal',\n",
       "  'classifier__loss': 'hinge'},\n",
       " {'accuracy': 0.9636129348959931,\n",
       "  'precision': np.float64(0.8553615960099751),\n",
       "  'recall': np.float64(0.7547940899088337),\n",
       "  'f1': np.float64(0.801937207748831),\n",
       "  'confusion_matrix': array([[ 2401,   780],\n",
       "         [  406, 29007]])},\n",
       " {'accuracy': 0.9616320687186829,\n",
       "  'precision': np.float64(0.8404605263157895),\n",
       "  'recall': np.float64(0.749266862170088),\n",
       "  'f1': np.float64(0.7922480620155039),\n",
       "  'confusion_matrix': array([[ 1022,   342],\n",
       "         [  194, 12412]])})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = data[tx_features + agg_features]\n",
    "\n",
    "# Split (sin escalar afuera)\n",
    "X_train_all, X_test_all, y_train, y_test = train_test_split(\n",
    "    X_all, y, test_size=0.3, random_state=10, stratify=y\n",
    ")\n",
    "\n",
    "# ========= ColumnTransformer =========\n",
    "numeric_features = X_all.columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features)\n",
    "])\n",
    "\n",
    "# ========= Pipeline base =========\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(random_state=10))\n",
    "])\n",
    "\n",
    "# ========= Hiperpar√°metros =========\n",
    "param_grid = {\n",
    "    'classifier__loss': ['log_loss', 'hinge', 'modified_huber'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "    'classifier__eta0': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# ========= GridSearchCV =========\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_all, y_train)\n",
    "\n",
    "# Resultados: f1 promedio y varianza\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "\n",
    "# Resultados\n",
    "results_table = pd.DataFrame(params)\n",
    "results_table['mean_f1'] = means\n",
    "results_table['std_f1'] = stds\n",
    "\n",
    "# Mejor configuraci√≥n: entrenando al mejor modelo\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Evaluaci√≥n\n",
    "y_train_pred = best_estimator.predict(X_train_all)\n",
    "y_test_pred = best_estimator.predict(X_test_all)\n",
    "\n",
    "\n",
    "results_train = metrics(y_train, y_train_pred)\n",
    "results_test = metrics(y_test, y_test_pred)\n",
    "\n",
    "results_table.sort_values(by='mean_f1', ascending=False).head(10), best_params, results_train, results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee2e7280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__alpha': 0.0001, 'classifier__eta0': 0.001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "# imprimir hiperpar√°metros del mejor modelo\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce0871c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluaci√≥n en conjunto de TEST (modelo final)\n",
      "Accuracy: 0.9616320687186829\n",
      "Precision: 0.8404605263157895\n",
      "Recall: 0.749266862170088\n",
      "F1 Score: 0.7922480620155039\n",
      "Matriz de confusi√≥n:\n",
      " [[ 1022   342]\n",
      " [  194 12412]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo entrenado\n",
    "print(\"\\nüìä Evaluaci√≥n en conjunto de TEST (modelo final)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred))\n",
    "print(\"Matriz de confusi√≥n:\\n\", confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2f9d2",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n \n",
    "El mejor modelo encontrado es un clasificador tipo SVM lineal (SGDClassifier(loss='hinge')) entrenado con SGD, con baja regularizaci√≥n (alpha = 0.0001) y un learning rate adaptativo.\n",
    "\n",
    "Se obtiene un F1 score de 0.79, indicando un buen equilibrio entre recall y precisi√≥n. \n",
    "\n",
    "El modelo tiene un recall de 0.75, indicando que se detectan tres cuartas partes de los casos de fraude. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b45848",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef24db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåü Evaluaci√≥n con TODAS LAS FEATURES (Random Forest)\n",
      "\n",
      "üå≤ Modelo 1 - Config: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2}\n",
      "Accuracy: 0.9843 | Precision: 0.9957 | Recall: 0.8424 | F1: 0.9126\n",
      "Matriz de confusi√≥n:\n",
      " [[ 1149   215]\n",
      " [    5 12601]] \n",
      "\n",
      "üå≤ Modelo 2 - Config: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n",
      "Accuracy: 0.9873 | Precision: 0.9966 | Recall: 0.8724 | F1: 0.9304\n",
      "Matriz de confusi√≥n:\n",
      " [[ 1190   174]\n",
      " [    4 12602]] \n",
      "\n",
      "üå≤ Modelo 3 - Config: {'n_estimators': 150, 'max_depth': None, 'min_samples_split': 2}\n",
      "Accuracy: 0.9875 | Precision: 0.9958 | Recall: 0.8754 | F1: 0.9317\n",
      "Matriz de confusi√≥n:\n",
      " [[ 1194   170]\n",
      " [    5 12601]] \n",
      "\n",
      "üå≤ Modelo 4 - Config: {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 10}\n",
      "Accuracy: 0.9864 | Precision: 0.9966 | Recall: 0.8636 | F1: 0.9254\n",
      "Matriz de confusi√≥n:\n",
      " [[ 1178   186]\n",
      " [    4 12602]] \n",
      "\n",
      "üå≤ Modelo 5 - Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 3}\n",
      "Accuracy: 0.9873 | Precision: 0.9958 | Recall: 0.8732 | F1: 0.9305\n",
      "Matriz de confusi√≥n:\n",
      " [[ 1191   173]\n",
      " [    5 12601]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>config</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_1</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 10, 'min_sam...</td>\n",
       "      <td>0.984252</td>\n",
       "      <td>0.995667</td>\n",
       "      <td>0.842375</td>\n",
       "      <td>0.912629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF_2</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 20, 'min_sa...</td>\n",
       "      <td>0.987258</td>\n",
       "      <td>0.996650</td>\n",
       "      <td>0.872434</td>\n",
       "      <td>0.930414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF_3</td>\n",
       "      <td>{'n_estimators': 150, 'max_depth': None, 'min_...</td>\n",
       "      <td>0.987473</td>\n",
       "      <td>0.995830</td>\n",
       "      <td>0.875367</td>\n",
       "      <td>0.931721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF_4</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 15, 'min_sa...</td>\n",
       "      <td>0.986399</td>\n",
       "      <td>0.996616</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.925373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF_5</td>\n",
       "      <td>{'n_estimators': 200, 'max_depth': 30, 'min_sa...</td>\n",
       "      <td>0.987258</td>\n",
       "      <td>0.995819</td>\n",
       "      <td>0.873167</td>\n",
       "      <td>0.930469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  modelo                                             config  accuracy  \\\n",
       "0   RF_1  {'n_estimators': 50, 'max_depth': 10, 'min_sam...  0.984252   \n",
       "1   RF_2  {'n_estimators': 100, 'max_depth': 20, 'min_sa...  0.987258   \n",
       "2   RF_3  {'n_estimators': 150, 'max_depth': None, 'min_...  0.987473   \n",
       "3   RF_4  {'n_estimators': 100, 'max_depth': 15, 'min_sa...  0.986399   \n",
       "4   RF_5  {'n_estimators': 200, 'max_depth': 30, 'min_sa...  0.987258   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.995667  0.842375  0.912629  \n",
       "1   0.996650  0.872434  0.930414  \n",
       "2   0.995830  0.875367  0.931721  \n",
       "3   0.996616  0.863636  0.925373  \n",
       "4   0.995819  0.873167  0.930469  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Etiquetas y features combinadas\n",
    "y = data['label'].astype(int)\n",
    "X = data[tx_features + agg_features]\n",
    "\n",
    "# Divisi√≥n (sin escalado)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10, stratify=y)\n",
    "\n",
    "# Configuraciones a probar\n",
    "configs = [\n",
    "    {\"n_estimators\": 50, \"max_depth\": 10, \"min_samples_split\": 2},\n",
    "    {\"n_estimators\": 100, \"max_depth\": 20, \"min_samples_split\": 5},\n",
    "    {\"n_estimators\": 150, \"max_depth\": None, \"min_samples_split\": 2},\n",
    "    {\"n_estimators\": 100, \"max_depth\": 15, \"min_samples_split\": 10},\n",
    "    {\"n_estimators\": 200, \"max_depth\": 30, \"min_samples_split\": 3},\n",
    "]\n",
    "\n",
    "# Evaluar Random Forest para cada configuraci√≥n\n",
    "resultados = []\n",
    "\n",
    "print(\"\\nüåü Evaluaci√≥n con TODAS LAS FEATURES (Random Forest)\\n\")\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=config['n_estimators'],\n",
    "        max_depth=config['max_depth'],\n",
    "        min_samples_split=config['min_samples_split'],\n",
    "        random_state=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"üå≤ Modelo {i+1} - Config: {config}\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    print(\"Matriz de confusi√≥n:\\n\", cm, \"\\n\")\n",
    "\n",
    "    resultados.append({\n",
    "        \"modelo\": f\"RF_{i+1}\",\n",
    "        \"config\": config,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1\n",
    "    })\n",
    "\n",
    "# Mostrar resumen final\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "display(df_resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c8cb4",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n \n",
    "\n",
    "Se entrenaron 5 modelos `RandomForestClassifier` con diferentes combinaciones de hiperpar√°metros, utilizando **todas las features disponibles** (transaccionales + agregadas), sin aplicar normalizaci√≥n ya que no es necesaria en este caso.\n",
    "\n",
    "Las combinaciones evaluadas incluyeron variaciones en:\n",
    "- `n_estimators` (cantidad de √°rboles en el bosque)\n",
    "- `max_depth` (profundidad m√°xima por √°rbol)\n",
    "- `min_samples_split` (m√≠nimo de muestras para dividir un nodo)\n",
    "\n",
    "Todos tuvieron muy buenos resultados, destac√°ndose:\n",
    "| RF_3 (150 √°rboles, sin l√≠mite de profundidad) |\n",
    "\n",
    "Este modelo logr√≥ el **mayor F1 score**, lo cual indica un equilibrio √≥ptimo entre:\n",
    "- **Precision muy alta** (casi no hay falsos positivos)\n",
    "- **Recall elevado** (detecta la mayor√≠a de los fraudes reales)\n",
    "\n",
    "\n",
    "En comparaci√≥n con modelos lineales como `SGDClassifier` (SVM lineal), Random Forest mostr√≥ un rendimiento superior, especialmente en **recall** y **F1 score**. Esto se debe a que:\n",
    "\n",
    "- Random Forest **no asume relaciones lineales** entre variables, por lo que puede capturar patrones m√°s complejos en los datos.\n",
    "- Al combinar m√∫ltiples √°rboles, reduce el sobreajuste y generaliza mejor.\n",
    "- Es menos sensible a la escala de los datos y al ruido, lo que simplifica el preprocesamiento.\n",
    "- Se adapta naturalmente a problemas con m√∫ltiples interacciones entre variables, como ocurre en la detecci√≥n de fraude.\n",
    "\n",
    "## Conclusi√≥n\n",
    "\n",
    "Random Forest demostr√≥ ser una **l√≠nea base s√≥lida** para abordar la clasificaci√≥n de fraudes con este dataset. El modelo `RF_3` ser√° utilizado como referencia para futuras entregas, en las que se explorar√°n enfoques m√°s avanzados como Graph Neural Networks (GNNs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71ece7",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "\n",
    "1. M. Weber, G. Domeniconi, J. Chen, D. K. I. Weidele, C. Bellei, T. Robinson, C. E. Leiserson, \"Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics\", KDD ‚Äô19 Workshop on Anomaly Detection in Finance, August 2019, Anchorage, AK, USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
