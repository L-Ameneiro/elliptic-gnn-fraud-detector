{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b8d0c6",
   "metadata": {},
   "source": [
    "## **Practico 2: Entrenamos modelos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3b5dc",
   "metadata": {},
   "source": [
    "1) A partir del grafo de elliptic dataset provisto en la librería torch-geometric, generar una versión de tabla csv, a partir de la cual se podría haber construido el mismo. Comprender en profundidad como se construye un grafo a partir de los datos de las transacciones y viceversa.\n",
    "\n",
    "2) (Tarea mas importante) Tomar como punto de partida el elliptic dataset provisto en la librería torch-geometric y comenzar una prueba de concepto buscando entrenar un modelo de machine learning capaz de predecir el fraude. Pueden trabajar con el archivo en formato grafo o csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9753d",
   "metadata": {},
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ba1f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import torch_geometric\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1793466",
   "metadata": {},
   "source": [
    "## 1- Generar CSV a partir de un grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4316657",
   "metadata": {},
   "source": [
    "Se comienza por importar el grafo de la librería torch-geometric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd4d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "\n",
    "dataset = EllipticBitcoinDataset(root='data/elliptic')\n",
    "data = dataset[0]  # Es un solo grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2175d7c",
   "metadata": {},
   "source": [
    "Se hecha un vistazo a los principales atributos de dicho grafo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcec0e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atributos disponibles en el grafo:\n",
      "['edge_index', 'x', 'test_mask', 'y', 'train_mask']\n"
     ]
    }
   ],
   "source": [
    "# Mostrar todos los atributos disponibles\n",
    "print(\"Atributos disponibles en el grafo:\")\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615155ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de features (x): torch.Size([203769, 165])\n",
      "Aristas (edge_index): torch.Size([2, 234355])\n",
      "Etiquetas (y): torch.Size([203769])\n"
     ]
    }
   ],
   "source": [
    "# Ver tamaños individuales\n",
    "print(\"Matriz de features (x):\", data.x.shape)\n",
    "print(\"Aristas (edge_index):\", data.edge_index.shape)\n",
    "print(\"Etiquetas (y):\", data.y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08d47ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodos: 203769\n",
      "Aristas: 234355\n"
     ]
    }
   ],
   "source": [
    "# Otras características\n",
    "print(\"Nodos:\", data.num_nodes)\n",
    "print(\"Aristas:\", data.num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88867a3",
   "metadata": {},
   "source": [
    "El grafo provisto por la librería `torch_geometric` contiene toda la información necesaria para su construcción. Desde el código, podemos explorar fácilmente los atributos del objeto `data` (el grado en sí):\n",
    "\n",
    "- `data.x`: matriz de características de cada nodo, con 166 features por transacción\n",
    "- `data.y`: etiquetas de cada nodo (0 = legítimo, 1 = fraude, 2 = desconocido)\n",
    "- `data.edge_index`: matriz con las conexiones entre nodos (aristas del grafo)\n",
    "- `data.num_nodes`: cantidad total de nodos (transacciones)\n",
    "- `data.num_edges`: cantidad total de aristas (conexiones entre transacciones)\n",
    "\n",
    "Como puede verse, estos datos están organizados en estructuras separadas, y no es posible representar **toda esta información** de forma fiel en un solo archivo `.csv` plano sin perder relaciones o duplicar contenido innecesariamente.\n",
    "\n",
    "Por eso, se opta por generar dos archivos `.csv`:\n",
    "1. `nodes.csv`: contiene una fila por transacción, con su ID, etiqueta y features.\n",
    "2. `edges.csv`: contiene una fila por relación, indicando cómo se conectan dos transacciones (nodos).\n",
    "\n",
    "Esta separación permite reconstruir el grafo completo y representa correctamente la naturaleza del dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9c8e5",
   "metadata": {},
   "source": [
    "Generar `nodes.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5a70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_155</th>\n",
       "      <th>feature_156</th>\n",
       "      <th>feature_157</th>\n",
       "      <th>feature_158</th>\n",
       "      <th>feature_159</th>\n",
       "      <th>feature_160</th>\n",
       "      <th>feature_161</th>\n",
       "      <th>feature_162</th>\n",
       "      <th>feature_163</th>\n",
       "      <th>feature_164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.171469</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562153</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947382</td>\n",
       "      <td>0.673103</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670883</td>\n",
       "      <td>0.439728</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782743</td>\n",
       "      <td>12.414557</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511871</td>\n",
       "      <td>-0.400422</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   node_id  fraud_label  feature_0  feature_1  feature_2  feature_3  \\\n",
       "0        0            2  -0.171469  -0.184668  -1.201369  -0.121970   \n",
       "1        1            2  -0.171484  -0.184668  -1.201369  -0.121970   \n",
       "2        2            2  -0.172107  -0.184668  -1.201369  -0.121970   \n",
       "3        3            0   0.163054   1.963790  -0.646376  12.409294   \n",
       "4        4            2   1.011523  -0.081127  -1.201369   1.153668   \n",
       "\n",
       "   feature_4  feature_5  feature_6  feature_7  ...  feature_155  feature_156  \\\n",
       "0  -0.043875  -0.113002  -0.061584  -0.162097  ...    -0.562153    -0.600999   \n",
       "1  -0.043875  -0.113002  -0.061584  -0.162112  ...     0.947382     0.673103   \n",
       "2  -0.043875  -0.113002  -0.061584  -0.162749  ...     0.670883     0.439728   \n",
       "3  -0.063725   9.782743  12.414557  -0.163645  ...    -0.577099    -0.613614   \n",
       "4   0.333276   1.312656  -0.061584  -0.163523  ...    -0.511871    -0.400422   \n",
       "\n",
       "   feature_157  feature_158  feature_159  feature_160  feature_161  \\\n",
       "0     1.461330     1.461369     0.018279    -0.087490    -0.131155   \n",
       "1    -0.979074    -0.978556     0.018279    -0.087490    -0.131155   \n",
       "2    -0.979074    -0.978556    -0.098889    -0.106715    -0.131155   \n",
       "3     0.241128     0.241406     1.072793     0.085530    -0.131155   \n",
       "4     0.517257     0.579382     0.018279     0.277775     0.326394   \n",
       "\n",
       "   feature_162  feature_163  feature_164  \n",
       "0    -0.097524    -0.120613    -0.119792  \n",
       "1    -0.097524    -0.120613    -0.119792  \n",
       "2    -0.183671    -0.120613    -0.119792  \n",
       "3     0.677799    -0.120613    -0.119792  \n",
       "4     1.293750     0.178136     0.179117  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Features por transacción (nodo)\n",
    "features = data.x.numpy()  # shape: (203769, 166)\n",
    "\n",
    "# Etiquetas: 0 = legítimo, 1 = fraude, 2 = desconocido\n",
    "labels = data.y.numpy()  # shape: (203769,)\n",
    "\n",
    "# Construir DataFrame\n",
    "df_nodes = pd.DataFrame(features, columns=[f'feature_{i}' for i in range(features.shape[1])])\n",
    "df_nodes.insert(0, 'fraud_label', labels)\n",
    "df_nodes.insert(0, 'node_id', range(data.num_nodes))\n",
    "\n",
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc06360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar\n",
    "df_nodes.to_csv(\"nodes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9973c3",
   "metadata": {},
   "source": [
    "Generar `edges.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e881244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from_node</th>\n",
       "      <th>to_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from_node  to_node\n",
       "0          0        1\n",
       "1          2        3\n",
       "2          4        5\n",
       "3          6        7\n",
       "4          8        9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aristas (relaciones entre transacciones)\n",
    "edge_index = data.edge_index.numpy()  # shape: (2, 234355)\n",
    "\n",
    "df_edges = pd.DataFrame({\n",
    "    'from_node': edge_index[0],\n",
    "    'to_node': edge_index[1]\n",
    "})\n",
    "\n",
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5ed9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar\n",
    "df_edges.to_csv(\"edges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb923cb8",
   "metadata": {},
   "source": [
    "Ambos `csv` generados serán suficientes para realizar el camino inverso y lograr construir un grafo de transacciones a partir de ellos, dando a entender que tanto una tabla como un grafo son 2 maneras de representar lo mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb2c7c",
   "metadata": {},
   "source": [
    "### Descripción del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e3d55",
   "metadata": {},
   "source": [
    "Continuaremos trabajando con Elliptic Data Set, presentado por Weber et al. en 2019 [1], el cual contiene datos anonimizados de 203,769 transacciones de Bitcoin. De la documentación del dataset sabemos que:\n",
    "\n",
    "2% de las transacciones están clasificadas como ilícitas (clase 1), 21% como lícitas (clase 2) y el 77% restante no ha sido etiquetado (clase 0). \n",
    "\n",
    "#### Features \n",
    "Cada transacción corresponde a un intervalo temporal de 3 hs (time step). Los time step son índices del 1 al 49 y están espaciados entre sí por dos semanas. Cada time step contiene un componente de transacciones conectadas entre sí, y desconectadas del resto de los componentes. \n",
    "\n",
    "De cada transacción se incluyen también 166 variables (features) cuya descripción exacta no es provista, pero se dan rasgos generales:\n",
    "\n",
    "##### Features directas de la transacción (1 a 94)\n",
    "Las primeras 94 features representan información directa de la transacción. Esto incluye al time-step, la cantidad de inputs/outputs, la tarifa de transacción, el volumen de output, así como variables agregadas tales como la cantidad promedio de BTC recibido (gastado) por los inputs/outputs y el número promedio de transacciones entrantes asociadas con los inputs/outpus.\n",
    "\n",
    "Estas son estadísticas directamente relacionadas con:\n",
    "- Cómo se construyó la transacción\n",
    "- De qué magnitud es\n",
    "- Qué tan activa es la wallet involucrada\n",
    "- Cuánto dinero se movió\n",
    "\n",
    "##### Features agregadas de transacciones vecinas (95 a 166)\n",
    "Las 72 features restantes provee estadísticas agregadas de las transacciones vecinas (una arista de distancia). Para cada variable incluida en las features directas, se calculan agregados (máximo, mínimo, desvío estándar y coeficientes de correlación) de las transacciones vecinas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86806187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los archivos\n",
    "features = pd.read_csv(r\"D:\\Documents\\Cursos\\Diplomatura FAMAF\\Mentoria\\elliptic-gnn-fraud-detector\\data\\elliptic\\elliptic_txs_features.csv\", header=None)\n",
    "edges = pd.read_csv(r\"D:\\Documents\\Cursos\\Diplomatura FAMAF\\Mentoria\\elliptic-gnn-fraud-detector\\data\\elliptic\\elliptic_txs_edgelist.csv\")\n",
    "classes = pd.read_csv(r\"D:\\Documents\\Cursos\\Diplomatura FAMAF\\Mentoria\\elliptic-gnn-fraud-detector\\data\\elliptic\\elliptic_txs_classes.csv\")\n",
    "\n",
    "# Renombrar columnas para claridad\n",
    "# features = features.rename(columns={0: \"txId\", 1: \"time_step\"})\n",
    "edges = edges.rename(columns={\"txId1\": \"source\", \"txId2\": \"target\"})\n",
    "classes = classes.rename(columns={\"txId\": \"txId\", \"class\": \"label\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4309caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renombrar columnas para mayor claridad\n",
    "tx_features = [\"tx_feat_\"+str(i) for i in range(2,95)]\n",
    "agg_features = [\"agg_feat_\"+str(i) for i in range(1,73)]\n",
    "features.columns = [\"txId\",\"time_step\"] + tx_features + agg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95950b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "    Features :  203,769 (rows)   167 (cols)\n",
      "    Classes  :  203,769 (rows)     2 (cols)\n",
      "    Edgelist :  234,355 (rows)     2 (cols)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Shapes\n",
    "{4*' '}Features : {features.shape[0]:8,} (rows)  {features.shape[1]:4,} (cols)\n",
    "{4*' '}Classes  : {classes.shape[0]:8,} (rows)  {classes.shape[1]:4,} (cols)\n",
    "{4*' '}Edgelist : {edges.shape[0]:8,} (rows)  {edges.shape[1]:4,} (cols)\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa83d4",
   "metadata": {},
   "source": [
    "#### Armado de dataset para modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acd68c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with classes\n",
    "df_features = pd.merge(features, classes, left_on='txId', right_on='txId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bc7f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['label'] = df_features['label'].apply(lambda x: '0' if x == \"unknown\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "883891df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    157205\n",
       "1      4545\n",
       "2     42019\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chequeamos que las clases nos quedaron bien armadas\n",
    "df_features.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cba84",
   "metadata": {},
   "source": [
    "## 2 - Entrenamiento de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd282713",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbe7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0),\n",
       " txId           0\n",
       " time_step      0\n",
       " tx_feat_2      0\n",
       " tx_feat_3      0\n",
       " tx_feat_4      0\n",
       "               ..\n",
       " agg_feat_69    0\n",
       " agg_feat_70    0\n",
       " agg_feat_71    0\n",
       " agg_feat_72    0\n",
       " label          0\n",
       " Length: 168, dtype: int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de filas con al menos un valor nulo\n",
    "num_rows_with_nan = df_features.isnull().any(axis=1).sum()\n",
    "\n",
    "# Cantidad de datos faltantes por variable\n",
    "missing_per_col = df_features.isnull().sum()\n",
    "\n",
    "(num_rows_with_nan, missing_per_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a42a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nos quedamos con datos etiquetados\n",
    "data = df_features[(df_features['label']=='1') | (df_features['label']=='2')]\n",
    "\n",
    "# creamos series para modelado\n",
    "X = data[['time_step'] + tx_features+agg_features]\n",
    "y = data['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56dfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# división entre entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a4f4b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución original: label\n",
      "2    0.902392\n",
      "1    0.097608\n",
      "Name: proportion, dtype: float64\n",
      "Train: label\n",
      "2    0.902123\n",
      "1    0.097877\n",
      "Name: proportion, dtype: float64\n",
      "Test: label\n",
      "2    0.903468\n",
      "1    0.096532\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distribución de variable target en subconjuntos de train y test\n",
    "print(\"Distribución original:\", y.value_counts(normalize=True))\n",
    "print(\"Train:\", y_train.value_counts(normalize=True))\n",
    "print(\"Test:\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21bd6b",
   "metadata": {},
   "source": [
    "Se observa una correcta distirbución entre los subconjuntos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b58348",
   "metadata": {},
   "source": [
    "### Baseline model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97d340",
   "metadata": {},
   "source": [
    "Se propone establecer un baseline score con un modelo simple, y luego intentamos superarlo (‘break the baseline’) con modelos más sofisticados.\n",
    "\n",
    "Para ello se utilizará DummyClassifier de sklearn con (strategy=\"stratified\") que predice al azar, pero manteniendo las proporciones de clases del conjunto de entrenamiento. Esto último es importante ya que se tiene un conjunto de datos fuertemente desbalanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1514c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Crear el modelo baseline\n",
    "dummy = DummyClassifier(strategy='stratified', random_state=10)  # estrategia: predecir al azar manteniendo las proporciones\n",
    "\n",
    "# 2. Entrenarlo con el conjunto de entrenamiento\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predecir sobre el conjunto de test\n",
    "y_pred = dummy.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1172d1b",
   "metadata": {},
   "source": [
    "Se defininen métricas para evaluar el modelo (y los siguientes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e13b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Resultados usando DummyClassifier (baseline)\n",
      "Accuracy: 0.8244389562976484\n",
      "Precision: 0.083710407239819\n",
      "Recall: 0.08231368186874305\n",
      "F1 score: 0.08300616937745373\n",
      "Matriz de confusión:\n",
      " [[  74  825]\n",
      " [ 810 7604]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n📊 Resultados usando DummyClassifier (baseline)\")\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",    recall_score(y_test, y_pred))\n",
    "print(\"F1 score:\",  f1_score(y_test, y_pred))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119d685",
   "metadata": {},
   "source": [
    "### Interpretación\n",
    "Si bien los resultados presentan una buena accuracy, ese valor es engañoso debido al desbalance del dataset.\n",
    "\n",
    "De ahi en mas, todas las métricas son muy bajas incluyendo un F1 pobre, lo cual es correcto, ya que ahora se tiene un baseline para romper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1dcf1",
   "metadata": {},
   "source": [
    "## Modelos lineales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cfed0",
   "metadata": {},
   "source": [
    "### Evaluando el poder explicativo de tx_features y agg_features\n",
    "\n",
    "En este conjuntos de datos se cuenta con dos grupos de features: uno que corresponde a características de cada transaccion y otro que corresponde a características de su contexto inmediato. \n",
    "Para evaluar el valor explicativo de cada grupo de features, entrenamos tres tipos de modelos:\n",
    "- dos modelos parciales: sólo tx_features (incluye time-step) y sólo agg_features. \n",
    "- un modelo completo (all_features)\n",
    "\n",
    "Realizamos este análisis tanto con modelos de clasificación por regresión logística como por descenso de gradiente.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44992e",
   "metadata": {},
   "source": [
    "#### Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47978afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Resultados usando solo features de transacción\n",
      "Accuracy: 0.9076592698639943\n",
      "Precision: 0.6703296703296703\n",
      "Recall: 0.13232104121475055\n",
      "F1 score: 0.2210144927536232\n",
      "Matriz de confusión:\n",
      " [[  183  1200]\n",
      " [   90 12497]]\n",
      "\n",
      "🔹 Resultados usando solo features agregadas\n",
      "Accuracy: 0.916034359341446\n",
      "Precision: 0.7536231884057971\n",
      "Recall: 0.22559652928416485\n",
      "F1 score: 0.34724540901502504\n",
      "Matriz de confusión:\n",
      " [[  312  1071]\n",
      " [  102 12485]]\n",
      "\n",
      "🔹 Resultados usando todas las features\n",
      "Accuracy: 0.9584108804581245\n",
      "Precision: 0.8369747899159664\n",
      "Recall: 0.720173535791757\n",
      "F1 score: 0.7741935483870968\n",
      "Matriz de confusión:\n",
      " [[  996   387]\n",
      " [  194 12393]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = data['label'].astype(int)\n",
    "\n",
    "# División en train/test\n",
    "X_tx = data[tx_features]\n",
    "X_agg = data[agg_features]\n",
    "X_all = data[tx_features + agg_features]\n",
    "\n",
    "X_train_tx, X_test_tx, y_train, y_test = train_test_split(X_tx, y, test_size=0.3, random_state=10)\n",
    "X_train_agg, X_test_agg = train_test_split(X_agg, test_size=0.3, random_state=10)\n",
    "X_train_all, X_test_all = train_test_split(X_all, test_size=0.3, random_state=10)\n",
    "\n",
    "# Escalado (opcional pero recomendado)\n",
    "scaler_tx = StandardScaler().fit(X_train_tx)\n",
    "scaler_agg = StandardScaler().fit(X_train_agg)\n",
    "scaler_all = StandardScaler().fit(X_train_all)\n",
    "\n",
    "X_train_tx = scaler_tx.transform(X_train_tx)\n",
    "X_test_tx = scaler_tx.transform(X_test_tx)\n",
    "\n",
    "X_train_agg = scaler_agg.transform(X_train_agg)\n",
    "X_test_agg = scaler_agg.transform(X_test_agg)\n",
    "\n",
    "X_train_all = scaler_all.transform(X_train_all)\n",
    "X_test_all = scaler_all.transform(X_test_all)\n",
    "\n",
    "# Entrenar modelos\n",
    "def eval_model(X_train, y_train, X_test, y_test, name):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\n🔹 Resultados usando {name}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Evaluar cada grupo de features\n",
    "eval_model(X_train_tx, y_train, X_test_tx, y_test, \"solo features de transacción\")\n",
    "eval_model(X_train_agg, y_train, X_test_agg, y_test, \"solo features agregadas\")\n",
    "eval_model(X_train_all, y_train, X_test_all, y_test, \"todas las features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e4e23",
   "metadata": {},
   "source": [
    "### Interpretación\n",
    "Los resultados muestran que, al estimar una clasificación por regresión lineal, tanto las características de la transacción como de su contexto inmediato tienen bajo valor predictivo, con F1 scores de 0.22 y 0.35 respectivamente. \n",
    "\n",
    "Ambos modelos parciales muestran muy baja recall, indicando que no están pudiendo detectar gran parte de los casos etiquetados como fraude. \n",
    "\n",
    "El modelo con todas las features tiene un mejor desempeño, evidenciado en su mayor score F1, de 0.77, que se debe sobre todo a su mejor valor de recall al reducir los falsos negativos (clave en este tipo de problemas de detección de fraude). Esto indica que la combinación de las features de la transacción y de su contexto son necesarias para predecir más eficazmente los casos de fraude. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d12c037",
   "metadata": {},
   "source": [
    "#### Clasificador por gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffca1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Resultados usando solo features de transacción (SGDClassifier)\n",
      "Accuracy: 0.9087329992841804\n",
      "Precision: 0.5973741794310722\n",
      "Recall: 0.2001466275659824\n",
      "F1 score: 0.29983525535420097\n",
      "Matriz de confusión:\n",
      " [[  273  1091]\n",
      " [  184 12422]]\n",
      "\n",
      "🔹 Resultados usando solo features agregadas (SGDClassifier)\n",
      "Accuracy: 0.9234073013600572\n",
      "Precision: 0.7370967741935484\n",
      "Recall: 0.3350439882697947\n",
      "F1 score: 0.46068548387096775\n",
      "Matriz de confusión:\n",
      " [[  457   907]\n",
      " [  163 12443]]\n",
      "\n",
      "🔹 Resultados usando todas las features (SGDClassifier)\n",
      "Accuracy: 0.9592698639942735\n",
      "Precision: 0.8545941123996432\n",
      "Recall: 0.7023460410557185\n",
      "F1 score: 0.7710261569416499\n",
      "Matriz de confusión:\n",
      " [[  958   406]\n",
      " [  163 12443]]\n"
     ]
    }
   ],
   "source": [
    "y = data['label'].astype(int)\n",
    "\n",
    "# Divisiones con estratificación\n",
    "X_tx = data[tx_features]\n",
    "X_agg = data[agg_features]\n",
    "X_all = data[tx_features + agg_features]\n",
    "\n",
    "X_train_tx, X_test_tx, y_train, y_test = train_test_split(\n",
    "    X_tx, y, test_size=0.3, random_state=10, stratify=y\n",
    ")\n",
    "X_train_agg, X_test_agg = train_test_split(\n",
    "    X_agg, test_size=0.3, random_state=10, stratify=y\n",
    ")\n",
    "X_train_all, X_test_all = train_test_split(\n",
    "    X_all, test_size=0.3, random_state=10, stratify=y\n",
    ")\n",
    "\n",
    "# Escalado\n",
    "scaler_tx = StandardScaler().fit(X_train_tx)\n",
    "scaler_agg = StandardScaler().fit(X_train_agg)\n",
    "scaler_all = StandardScaler().fit(X_train_all)\n",
    "\n",
    "X_train_tx = scaler_tx.transform(X_train_tx)\n",
    "X_test_tx = scaler_tx.transform(X_test_tx)\n",
    "\n",
    "X_train_agg = scaler_agg.transform(X_train_agg)\n",
    "X_test_agg = scaler_agg.transform(X_test_agg)\n",
    "\n",
    "X_train_all = scaler_all.transform(X_train_all)\n",
    "X_test_all = scaler_all.transform(X_test_all)\n",
    "\n",
    "# Evaluación con SGDClassifier\n",
    "def eval_sgd(X_train, y_train, X_test, y_test, name):\n",
    "    clf = SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\n🔹 Resultados usando {name} (SGDClassifier)\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Ejecutar evaluaciones\n",
    "eval_sgd(X_train_tx, y_train, X_test_tx, y_test, \"solo features de transacción\")\n",
    "eval_sgd(X_train_agg, y_train, X_test_agg, y_test, \"solo features agregadas\")\n",
    "eval_sgd(X_train_all, y_train, X_test_all, y_test, \"todas las features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7889d2",
   "metadata": {},
   "source": [
    "### Interpretación\n",
    "Nuevamente verificamos que la combinación de features de la transacción y de su contexto es lo que mejor permite predecir los casos de fraude: se logra un buen nivel de recall, logrando predecir correctamente el 70% de los casos de fraude. Al mismo tiempo, se mantiene un buen nivel de precisión, lo que indica baja proporción de falsos positivos. \n",
    "\n",
    "Sin embargo, vemos que en este modelo ambos modelos parciales difieren notablemente en su capacidad predictiva, siendo las features del contexto las que explican más varianza y tienen mayor poder predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4eaccb",
   "metadata": {},
   "source": [
    "#### Buscando el mejor modelo por descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "536440dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas función\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b223fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    classifier__alpha  classifier__eta0 classifier__learning_rate  \\\n",
       " 4              0.0001             0.001                   optimal   \n",
       " 13             0.0001             0.010                   optimal   \n",
       " 22             0.0001             0.100                   optimal   \n",
       " 3              0.0001             0.001                   optimal   \n",
       " 12             0.0001             0.010                   optimal   \n",
       " 21             0.0001             0.100                   optimal   \n",
       " 23             0.0001             0.100                   optimal   \n",
       " 14             0.0001             0.010                   optimal   \n",
       " 5              0.0001             0.001                   optimal   \n",
       " 31             0.0010             0.001                   optimal   \n",
       " \n",
       "    classifier__loss   mean_f1    std_f1  \n",
       " 4             hinge  0.781892  0.038922  \n",
       " 13            hinge  0.781892  0.038922  \n",
       " 22            hinge  0.781892  0.038922  \n",
       " 3          log_loss  0.758008  0.006926  \n",
       " 12         log_loss  0.758008  0.006926  \n",
       " 21         log_loss  0.758008  0.006926  \n",
       " 23   modified_huber  0.747200  0.034520  \n",
       " 14   modified_huber  0.747200  0.034520  \n",
       " 5    modified_huber  0.747200  0.034520  \n",
       " 31            hinge  0.742883  0.005001  ,\n",
       " {'classifier__alpha': 0.0001,\n",
       "  'classifier__eta0': 0.001,\n",
       "  'classifier__learning_rate': 'optimal',\n",
       "  'classifier__loss': 'hinge'},\n",
       " {'accuracy': 0.9636129348959931,\n",
       "  'precision': np.float64(0.8553615960099751),\n",
       "  'recall': np.float64(0.7547940899088337),\n",
       "  'f1': np.float64(0.801937207748831),\n",
       "  'confusion_matrix': array([[ 2401,   780],\n",
       "         [  406, 29007]])},\n",
       " {'accuracy': 0.9616320687186829,\n",
       "  'precision': np.float64(0.8404605263157895),\n",
       "  'recall': np.float64(0.749266862170088),\n",
       "  'f1': np.float64(0.7922480620155039),\n",
       "  'confusion_matrix': array([[ 1022,   342],\n",
       "         [  194, 12412]])})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = data[tx_features + agg_features]\n",
    "\n",
    "# Split (sin escalar afuera)\n",
    "X_train_all, X_test_all, y_train, y_test = train_test_split(\n",
    "    X_all, y, test_size=0.3, random_state=10, stratify=y\n",
    ")\n",
    "\n",
    "# ========= ColumnTransformer =========\n",
    "numeric_features = X_all.columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features)\n",
    "])\n",
    "\n",
    "# ========= Pipeline base =========\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(random_state=10))\n",
    "])\n",
    "\n",
    "# ========= Hiperparámetros =========\n",
    "param_grid = {\n",
    "    'classifier__loss': ['log_loss', 'hinge', 'modified_huber'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "    'classifier__eta0': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# ========= GridSearchCV =========\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_all, y_train)\n",
    "\n",
    "# Resultados: f1 promedio y varianza\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "\n",
    "# Resultados\n",
    "results_table = pd.DataFrame(params)\n",
    "results_table['mean_f1'] = means\n",
    "results_table['std_f1'] = stds\n",
    "\n",
    "# Mejor configuración: entrenando al mejor modelo\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Evaluación\n",
    "y_train_pred = best_estimator.predict(X_train_all)\n",
    "y_test_pred = best_estimator.predict(X_test_all)\n",
    "\n",
    "\n",
    "results_train = metrics(y_train, y_train_pred)\n",
    "results_test = metrics(y_test, y_test_pred)\n",
    "\n",
    "results_table.sort_values(by='mean_f1', ascending=False).head(10), best_params, results_train, results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee2e7280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__alpha': 0.0001, 'classifier__eta0': 0.001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "# imprimir hiperparámetros del mejor modelo\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce0871c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluación en conjunto de TEST (modelo final)\n",
      "Accuracy: 0.9616320687186829\n",
      "Precision: 0.8404605263157895\n",
      "Recall: 0.749266862170088\n",
      "F1 Score: 0.7922480620155039\n",
      "Matriz de confusión:\n",
      " [[ 1022   342]\n",
      " [  194 12412]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo entrenado\n",
    "print(\"\\n📊 Evaluación en conjunto de TEST (modelo final)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2f9d2",
   "metadata": {},
   "source": [
    "### Interpretación \n",
    "El mejor modelo encontrado es un clasificador tipo SVM lineal (SGDClassifier(loss='hinge')) entrenado con SGD, con baja regularización (alpha = 0.0001) y un learning rate adaptativo.\n",
    "\n",
    "Se obtiene un F1 score de 0.79, indicando un buen equilibrio entre recall y precisión. \n",
    "\n",
    "El modelo tiene un recall de 0.75, indicando que se detectan tres cuartas partes de los casos de fraude. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b45848",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef24db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌟 Evaluación con TODAS LAS FEATURES (Random Forest)\n",
      "\n",
      "🌲 Modelo 1 - Config: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2}\n",
      "Accuracy: 0.9843 | Precision: 0.9957 | Recall: 0.8424 | F1: 0.9126\n",
      "Matriz de confusión:\n",
      " [[ 1149   215]\n",
      " [    5 12601]] \n",
      "\n",
      "🌲 Modelo 2 - Config: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n",
      "Accuracy: 0.9873 | Precision: 0.9966 | Recall: 0.8724 | F1: 0.9304\n",
      "Matriz de confusión:\n",
      " [[ 1190   174]\n",
      " [    4 12602]] \n",
      "\n",
      "🌲 Modelo 3 - Config: {'n_estimators': 150, 'max_depth': None, 'min_samples_split': 2}\n",
      "Accuracy: 0.9875 | Precision: 0.9958 | Recall: 0.8754 | F1: 0.9317\n",
      "Matriz de confusión:\n",
      " [[ 1194   170]\n",
      " [    5 12601]] \n",
      "\n",
      "🌲 Modelo 4 - Config: {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 10}\n",
      "Accuracy: 0.9864 | Precision: 0.9966 | Recall: 0.8636 | F1: 0.9254\n",
      "Matriz de confusión:\n",
      " [[ 1178   186]\n",
      " [    4 12602]] \n",
      "\n",
      "🌲 Modelo 5 - Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 3}\n",
      "Accuracy: 0.9873 | Precision: 0.9958 | Recall: 0.8732 | F1: 0.9305\n",
      "Matriz de confusión:\n",
      " [[ 1191   173]\n",
      " [    5 12601]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>config</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_1</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 10, 'min_sam...</td>\n",
       "      <td>0.984252</td>\n",
       "      <td>0.995667</td>\n",
       "      <td>0.842375</td>\n",
       "      <td>0.912629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF_2</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 20, 'min_sa...</td>\n",
       "      <td>0.987258</td>\n",
       "      <td>0.996650</td>\n",
       "      <td>0.872434</td>\n",
       "      <td>0.930414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF_3</td>\n",
       "      <td>{'n_estimators': 150, 'max_depth': None, 'min_...</td>\n",
       "      <td>0.987473</td>\n",
       "      <td>0.995830</td>\n",
       "      <td>0.875367</td>\n",
       "      <td>0.931721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF_4</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 15, 'min_sa...</td>\n",
       "      <td>0.986399</td>\n",
       "      <td>0.996616</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.925373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF_5</td>\n",
       "      <td>{'n_estimators': 200, 'max_depth': 30, 'min_sa...</td>\n",
       "      <td>0.987258</td>\n",
       "      <td>0.995819</td>\n",
       "      <td>0.873167</td>\n",
       "      <td>0.930469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  modelo                                             config  accuracy  \\\n",
       "0   RF_1  {'n_estimators': 50, 'max_depth': 10, 'min_sam...  0.984252   \n",
       "1   RF_2  {'n_estimators': 100, 'max_depth': 20, 'min_sa...  0.987258   \n",
       "2   RF_3  {'n_estimators': 150, 'max_depth': None, 'min_...  0.987473   \n",
       "3   RF_4  {'n_estimators': 100, 'max_depth': 15, 'min_sa...  0.986399   \n",
       "4   RF_5  {'n_estimators': 200, 'max_depth': 30, 'min_sa...  0.987258   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.995667  0.842375  0.912629  \n",
       "1   0.996650  0.872434  0.930414  \n",
       "2   0.995830  0.875367  0.931721  \n",
       "3   0.996616  0.863636  0.925373  \n",
       "4   0.995819  0.873167  0.930469  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Etiquetas y features combinadas\n",
    "y = data['label'].astype(int)\n",
    "X = data[tx_features + agg_features]\n",
    "\n",
    "# División (sin escalado)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10, stratify=y)\n",
    "\n",
    "# Configuraciones a probar\n",
    "configs = [\n",
    "    {\"n_estimators\": 50, \"max_depth\": 10, \"min_samples_split\": 2},\n",
    "    {\"n_estimators\": 100, \"max_depth\": 20, \"min_samples_split\": 5},\n",
    "    {\"n_estimators\": 150, \"max_depth\": None, \"min_samples_split\": 2},\n",
    "    {\"n_estimators\": 100, \"max_depth\": 15, \"min_samples_split\": 10},\n",
    "    {\"n_estimators\": 200, \"max_depth\": 30, \"min_samples_split\": 3},\n",
    "]\n",
    "\n",
    "# Evaluar Random Forest para cada configuración\n",
    "resultados = []\n",
    "\n",
    "print(\"\\n🌟 Evaluación con TODAS LAS FEATURES (Random Forest)\\n\")\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=config['n_estimators'],\n",
    "        max_depth=config['max_depth'],\n",
    "        min_samples_split=config['min_samples_split'],\n",
    "        random_state=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"🌲 Modelo {i+1} - Config: {config}\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    print(\"Matriz de confusión:\\n\", cm, \"\\n\")\n",
    "\n",
    "    resultados.append({\n",
    "        \"modelo\": f\"RF_{i+1}\",\n",
    "        \"config\": config,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1\n",
    "    })\n",
    "\n",
    "# Mostrar resumen final\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "display(df_resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c8cb4",
   "metadata": {},
   "source": [
    "### Interpretación \n",
    "\n",
    "Se entrenaron 5 modelos `RandomForestClassifier` con diferentes combinaciones de hiperparámetros, utilizando **todas las features disponibles** (transaccionales + agregadas), sin aplicar normalización ya que no es necesaria en este caso.\n",
    "\n",
    "Las combinaciones evaluadas incluyeron variaciones en:\n",
    "- `n_estimators` (cantidad de árboles en el bosque)\n",
    "- `max_depth` (profundidad máxima por árbol)\n",
    "- `min_samples_split` (mínimo de muestras para dividir un nodo)\n",
    "\n",
    "Todos tuvieron muy buenos resultados, destacándose:\n",
    "| RF_3 (150 árboles, sin límite de profundidad) |\n",
    "\n",
    "Este modelo logró el **mayor F1 score**, lo cual indica un equilibrio óptimo entre:\n",
    "- **Precision muy alta** (casi no hay falsos positivos)\n",
    "- **Recall elevado** (detecta la mayoría de los fraudes reales)\n",
    "\n",
    "\n",
    "En comparación con modelos lineales como `SGDClassifier` (SVM lineal), Random Forest mostró un rendimiento superior, especialmente en **recall** y **F1 score**. Esto se debe a que:\n",
    "\n",
    "- Random Forest **no asume relaciones lineales** entre variables, por lo que puede capturar patrones más complejos en los datos.\n",
    "- Al combinar múltiples árboles, reduce el sobreajuste y generaliza mejor.\n",
    "- Es menos sensible a la escala de los datos y al ruido, lo que simplifica el preprocesamiento.\n",
    "- Se adapta naturalmente a problemas con múltiples interacciones entre variables, como ocurre en la detección de fraude.\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "Random Forest demostró ser una **línea base sólida** para abordar la clasificación de fraudes con este dataset. El modelo `RF_3` será utilizado como referencia para futuras entregas, en las que se explorarán enfoques más avanzados como Graph Neural Networks (GNNs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71ece7",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "\n",
    "1. M. Weber, G. Domeniconi, J. Chen, D. K. I. Weidele, C. Bellei, T. Robinson, C. E. Leiserson, \"Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics\", KDD ’19 Workshop on Anomaly Detection in Finance, August 2019, Anchorage, AK, USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
