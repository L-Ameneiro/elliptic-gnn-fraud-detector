{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6ba1f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e6e02",
   "metadata": {},
   "source": [
    "Notebooks de referencia: (para borrar antes de entrega)\n",
    "\n",
    "https://www.kaggle.com/code/clemwo/bitcoin-transactions-graph-neural-networks\n",
    "\n",
    "https://www.kaggle.com/code/nidropakshin/ellptic-data-gat-tagnn\n",
    "\n",
    "https://www.kaggle.com/code/karthikapv/licit-illicit-logistic-regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb2c7c",
   "metadata": {},
   "source": [
    "### Descripción del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e3d55",
   "metadata": {},
   "source": [
    "Continuaremos trabajando con Elliptic Data Set, presentado por Weber et al. en 2019 [1], el cual contiene datos anonimizados de 203,769 transacciones de Bitcoin. De la documentación del dataset sabemos que:\n",
    "\n",
    "2% de las transacciones están clasificadas como ilícitas (clase 1), 21% como lícitas (clase 2) y el 77% restante no ha sido etiquetado (clase 0). \n",
    "\n",
    "#### Features \n",
    "Cada transacción corresponde a un intervalo temporal de 3 hs (time step). Los time step son índices del 1 al 49 y están espaciados entre sí por dos semanas. Cada time step contiene un componente de transacciones conectadas entre sí, y desconectadas del resto de los componentes. \n",
    "\n",
    "De cada transacción se incluyen también 166 variables (features) cuya descripción exacta no es provista, pero se dan rasgos generales:\n",
    "\n",
    "##### Features directas de la transacción (1 a 94)\n",
    "Las primeras 94 features representan información directa de la transacción. Esto incluye al time-step, la cantidad de inputs/outputs, la tarifa de transacción, el volumen de output, así como variables agregadas tales como la cantidad promedio de BTC recibido (gastado) por los inputs/outputs y el número promedio de transacciones entrantes asociadas con los inputs/outpus.\n",
    "\n",
    "Estas son estadísticas directamente relacionadas con:\n",
    "- Cómo se construyó la transacción\n",
    "- De qué magnitud es\n",
    "- Qué tan activa es la wallet involucrada\n",
    "- Cuánto dinero se movió\n",
    "\n",
    "##### Features agregadas de transacciones vecinas (95 a 166)\n",
    "Las 72 features restantes provee estadísticas agregadas de las transacciones vecinas (una arista de distancia). Para cada variable incluida en las features directas, se calculan agregados (máximo, mínimo, desvío estándar y coeficientes de correlación) de las transacciones vecinas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "86806187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los archivos\n",
    "features = pd.read_csv(\"../data/kaggle/elliptic_txs_features.csv\", header=None)\n",
    "edges = pd.read_csv(\"../data/kaggle/elliptic_txs_edgelist.csv\")\n",
    "classes = pd.read_csv(\"../data/kaggle/elliptic_txs_classes.csv\")\n",
    "\n",
    "# Renombrar columnas para claridad\n",
    "# features = features.rename(columns={0: \"txId\", 1: \"time_step\"})\n",
    "edges = edges.rename(columns={\"txId1\": \"source\", \"txId2\": \"target\"})\n",
    "classes = classes.rename(columns={\"txId\": \"txId\", \"class\": \"label\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4309caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renombrar columnas para mayor claridad\n",
    "tx_features = [\"tx_feat_\"+str(i) for i in range(2,95)]\n",
    "agg_features = [\"agg_feat_\"+str(i) for i in range(1,73)]\n",
    "features.columns = [\"txId\",\"time_step\"] + tx_features + agg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "95950b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "    Features :  203,769 (rows)   167 (cols)\n",
      "    Classes  :  203,769 (rows)     2 (cols)\n",
      "    Edgelist :  234,355 (rows)     2 (cols)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Shapes\n",
    "{4*' '}Features : {features.shape[0]:8,} (rows)  {features.shape[1]:4,} (cols)\n",
    "{4*' '}Classes  : {classes.shape[0]:8,} (rows)  {classes.shape[1]:4,} (cols)\n",
    "{4*' '}Edgelist : {edges.shape[0]:8,} (rows)  {edges.shape[1]:4,} (cols)\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa83d4",
   "metadata": {},
   "source": [
    "#### Armado de dataset para modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "acd68c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with classes\n",
    "df_features = pd.merge(features, classes, left_on='txId', right_on='txId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5bc7f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['label'] = df_features['label'].apply(lambda x: '0' if x == \"unknown\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "883891df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    157205\n",
       "1      4545\n",
       "2     42019\n",
       "dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chequeamos que las clases nos quedaron bien armadas\n",
    "df_features.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cba84",
   "metadata": {},
   "source": [
    "## Entrenamiento de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd282713",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7dbbe7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0),\n",
       " txId           0\n",
       " time_step      0\n",
       " tx_feat_2      0\n",
       " tx_feat_3      0\n",
       " tx_feat_4      0\n",
       "               ..\n",
       " agg_feat_69    0\n",
       " agg_feat_70    0\n",
       " agg_feat_71    0\n",
       " agg_feat_72    0\n",
       " label          0\n",
       " Length: 168, dtype: int64)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de filas con al menos un valor nulo\n",
    "num_rows_with_nan = df_features.isnull().any(axis=1).sum()\n",
    "\n",
    "# Cantidad de datos faltantes por variable\n",
    "missing_per_col = df_features.isnull().sum()\n",
    "\n",
    "(num_rows_with_nan, missing_per_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f0a42a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nos quedamos con datos etiquetados\n",
    "data = df_features[(df_features['label']=='1') | (df_features['label']=='2')]\n",
    "\n",
    "# creamos series para modelado\n",
    "X = data[['time_step'] + tx_features+agg_features]\n",
    "y = data['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a56dfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# división entre entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5a4f4b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución original: label\n",
      "2    0.902392\n",
      "1    0.097608\n",
      "Name: proportion, dtype: float64\n",
      "Train: label\n",
      "2    0.901452\n",
      "1    0.098548\n",
      "Name: proportion, dtype: float64\n",
      "Test: label\n",
      "2    0.906153\n",
      "1    0.093847\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distribución de variable target en subconjuntos de train y test\n",
    "print(\"Distribución original:\", y.value_counts(normalize=True))\n",
    "print(\"Train:\", y_train.value_counts(normalize=True))\n",
    "print(\"Test:\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620186c",
   "metadata": {},
   "source": [
    "## Modelos lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cfed0",
   "metadata": {},
   "source": [
    "### Evaluando el poder explicativo de tx_features y agg_features\n",
    "\n",
    "En este conjuntos de datos se cuenta con dos grupos de features: uno que corresponde a características de cada transaccion y otro que corresponde a características de su contexto inmediato. \n",
    "Para evaluar el valor explicativo de cada grupo de features, entrenamos tres tipos de modelos:\n",
    "- dos modelos parciales: sólo tx_features (incluye time-step) y sólo agg_features. \n",
    "- un modelo completo (all_features)\n",
    "\n",
    "Realizamos este análisis tanto con modelos de clasificación por regresión logística como por descenso de gradiente.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44992e",
   "metadata": {},
   "source": [
    "#### Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "47978afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Resultados usando solo features de transacción\n",
      "Accuracy: 0.9076592698639943\n",
      "Precision: 0.6678700361010831\n",
      "Recall: 0.13376717281272596\n",
      "F1 score: 0.22289156626506024\n",
      "Matriz de confusión:\n",
      " [[  185  1198]\n",
      " [   92 12495]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[176]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Evaluar cada grupo de features\u001b[39;00m\n\u001b[32m     39\u001b[39m eval_model(X_train_tx, y_train, X_test_tx, y_test, \u001b[33m\"\u001b[39m\u001b[33msolo features de transacción\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_agg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_agg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msolo features agregadas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m eval_model(X_train_all, y_train, X_test_all, y_test, \u001b[33m\"\u001b[39m\u001b[33mtodas las features\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[176]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36meval_model\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, name)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meval_model\u001b[39m(X_train, y_train, X_test, y_test, name):\n\u001b[32m     28\u001b[39m     clf = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     y_pred = clf.predict(X_test)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔹 Resultados usando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1384\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1382\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1409\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1410\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:459\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    455\u001b[39m l2_reg_strength = \u001b[32m1.0\u001b[39m / (C * sw_sum)\n\u001b[32m    456\u001b[39m iprint = [-\u001b[32m1\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m101\u001b[39m][\n\u001b[32m    457\u001b[39m     np.searchsorted(np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]), verbose)\n\u001b[32m    458\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m opt_res = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mftol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_get_additional_lbfgs_options_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miprint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m n_iter_i = _check_optimize_result(\n\u001b[32m    474\u001b[39m     solver,\n\u001b[32m    475\u001b[39m     opt_res,\n\u001b[32m    476\u001b[39m     max_iter,\n\u001b[32m    477\u001b[39m     extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[32m    478\u001b[39m )\n\u001b[32m    479\u001b[39m w0, loss = opt_res.x, opt_res.fun\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/scipy/optimize/_minimize.py:785\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    782\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    783\u001b[39m                              **options)\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    788\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    789\u001b[39m                         **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:469\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[39m\n\u001b[32m    461\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    462\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    472\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:403\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28mself\u001b[39m._update_grad()\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:353\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m         fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28mself\u001b[39m._nfev += \u001b[32m1\u001b[39m\n\u001b[32m    355\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fx < \u001b[38;5;28mself\u001b[39m._lowest_f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/scipy/_lib/_util.py:583\u001b[39m, in \u001b[36m_ScalarFunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    581\u001b[39m     \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    582\u001b[39m     \u001b[38;5;66;03m# The user of this class might want `x` to remain unchanged.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    584\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    586\u001b[39m     \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/scipy/optimize/_optimize.py:80\u001b[39m, in \u001b[36mMemoizeJac.__call__\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, *args):\n\u001b[32m     79\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/scipy/optimize/_optimize.py:74\u001b[39m, in \u001b[36mMemoizeJac._compute_if_needed\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(x == \u001b[38;5;28mself\u001b[39m.x) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.x = np.asarray(x).copy()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     fg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.jac = fg[\u001b[32m1\u001b[39m]\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._value = fg[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:332\u001b[39m, in \u001b[36mLinearModelLoss.loss_gradient\u001b[39m\u001b[34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[39m\n\u001b[32m    330\u001b[39m     grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit_intercept:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m         grad[-\u001b[32m1\u001b[39m] = \u001b[43mgrad_pointwise\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    334\u001b[39m     grad = np.empty((n_classes, n_dof), dtype=weights.dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DiploDatos/elliptic-gnn-fraud-detector/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:49\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_amin\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     46\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     50\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prod\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     54\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "y = data['label'].astype(int)\n",
    "\n",
    "# División en train/test\n",
    "X_tx = data[tx_features]\n",
    "X_agg = data[agg_features]\n",
    "X_all = data[tx_features + agg_features]\n",
    "\n",
    "X_train_tx, X_test_tx, y_train, y_test = train_test_split(X_tx, y, test_size=0.3, random_state=42)\n",
    "X_train_agg, X_test_agg = train_test_split(X_agg, test_size=0.3, random_state=42)\n",
    "X_train_all, X_test_all = train_test_split(X_all, test_size=0.3, random_state=42)\n",
    "\n",
    "# Escalado (opcional pero recomendado)\n",
    "scaler_tx = StandardScaler().fit(X_train_tx)\n",
    "scaler_agg = StandardScaler().fit(X_train_agg)\n",
    "scaler_all = StandardScaler().fit(X_train_all)\n",
    "\n",
    "X_train_tx = scaler_tx.transform(X_train_tx)\n",
    "X_test_tx = scaler_tx.transform(X_test_tx)\n",
    "\n",
    "X_train_agg = scaler_agg.transform(X_train_agg)\n",
    "X_test_agg = scaler_agg.transform(X_test_agg)\n",
    "\n",
    "X_train_all = scaler_all.transform(X_train_all)\n",
    "X_test_all = scaler_all.transform(X_test_all)\n",
    "\n",
    "# Entrenar modelos\n",
    "def eval_model(X_train, y_train, X_test, y_test, name):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\n🔹 Resultados usando {name}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Evaluar cada grupo de features\n",
    "eval_model(X_train_tx, y_train, X_test_tx, y_test, \"solo features de transacción\")\n",
    "eval_model(X_train_agg, y_train, X_test_agg, y_test, \"solo features agregadas\")\n",
    "eval_model(X_train_all, y_train, X_test_all, y_test, \"todas las features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e4e23",
   "metadata": {},
   "source": [
    "### Interpretación\n",
    "Los resultados muestran que, al estimar una clasificación por regresión lineal, tanto las características de la transacción como de su contexto inmediato tienen bajo valor predictivo, con F1 scores de 0.22 y 0.35 respectivamente. \n",
    "\n",
    "Ambos modelos parciales muestran muy baja recall, indicando que no están pudiendo detectar gran parte de los casos etiquetados como fraude. \n",
    "\n",
    "El modelo con todas las features tiene un mejor desempeño, evidenciado en su mayor score F1, de 0.77, que se debe sobre todo a su mejor valor de recall. Esto indica que la combinación de las features de la transacción y de su contexto son necesarias para predecir más eficazmente los casos de fraude. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d12c037",
   "metadata": {},
   "source": [
    "#### Clasificador por gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4ffca1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Resultados usando solo features de transacción (SGDClassifier)\n",
      "Accuracy: 0.9088761632068719\n",
      "Precision: 0.5986984815618221\n",
      "Recall: 0.20234604105571846\n",
      "F1 score: 0.30246575342465754\n",
      "Matriz de confusión:\n",
      " [[  276  1088]\n",
      " [  185 12421]]\n",
      "\n",
      "🔹 Resultados usando solo features agregadas (SGDClassifier)\n",
      "Accuracy: 0.9234073013600572\n",
      "Precision: 0.7370967741935484\n",
      "Recall: 0.3350439882697947\n",
      "F1 score: 0.46068548387096775\n",
      "Matriz de confusión:\n",
      " [[  457   907]\n",
      " [  163 12443]]\n",
      "\n",
      "🔹 Resultados usando todas las features (SGDClassifier)\n",
      "Accuracy: 0.9591982820329277\n",
      "Precision: 0.8538324420677362\n",
      "Recall: 0.7023460410557185\n",
      "F1 score: 0.7707160096540627\n",
      "Matriz de confusión:\n",
      " [[  958   406]\n",
      " [  164 12442]]\n"
     ]
    }
   ],
   "source": [
    "y = data['label'].astype(int)\n",
    "\n",
    "# Divisiones con estratificación\n",
    "X_tx = data[tx_features]\n",
    "X_agg = data[agg_features]\n",
    "X_all = data[tx_features + agg_features]\n",
    "\n",
    "X_train_tx, X_test_tx, y_train, y_test = train_test_split(\n",
    "    X_tx, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_train_agg, X_test_agg = train_test_split(\n",
    "    X_agg, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_train_all, X_test_all = train_test_split(\n",
    "    X_all, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Escalado\n",
    "scaler_tx = StandardScaler().fit(X_train_tx)\n",
    "scaler_agg = StandardScaler().fit(X_train_agg)\n",
    "scaler_all = StandardScaler().fit(X_train_all)\n",
    "\n",
    "X_train_tx = scaler_tx.transform(X_train_tx)\n",
    "X_test_tx = scaler_tx.transform(X_test_tx)\n",
    "\n",
    "X_train_agg = scaler_agg.transform(X_train_agg)\n",
    "X_test_agg = scaler_agg.transform(X_test_agg)\n",
    "\n",
    "X_train_all = scaler_all.transform(X_train_all)\n",
    "X_test_all = scaler_all.transform(X_test_all)\n",
    "\n",
    "# Evaluación con SGDClassifier\n",
    "def eval_sgd(X_train, y_train, X_test, y_test, name):\n",
    "    clf = SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\n🔹 Resultados usando {name} (SGDClassifier)\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Ejecutar evaluaciones\n",
    "eval_sgd(X_train_tx, y_train, X_test_tx, y_test, \"solo features de transacción\")\n",
    "eval_sgd(X_train_agg, y_train, X_test_agg, y_test, \"solo features agregadas\")\n",
    "eval_sgd(X_train_all, y_train, X_test_all, y_test, \"todas las features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7889d2",
   "metadata": {},
   "source": [
    "### Interpretación\n",
    "Nuevamente verificamos que la combinación de features de la transacción y de su contexto es lo que mejor permite predecir los casos de fraude: se logra un buen nivel de recall, logrando predecir correctamente el 70% de los casos de fraude. Al mismo tiempo, se mantiene un buen nivel de precisión, lo que indica baja proporción de falsos positivos. \n",
    "\n",
    "Sin embargo, vemos que en este modelo ambos modelos parciales difieren notablemente en su capacidad predictiva, siendo las features del contexto las que explican más varianza y tienen mayor poder predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4eaccb",
   "metadata": {},
   "source": [
    "#### Buscando el mejor modelo por descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "536440dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas función\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c2b223fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    classifier__alpha  classifier__eta0 classifier__learning_rate  \\\n",
       " 4              0.0001             0.001                   optimal   \n",
       " 13             0.0001             0.010                   optimal   \n",
       " 22             0.0001             0.100                   optimal   \n",
       " 3              0.0001             0.001                   optimal   \n",
       " 12             0.0001             0.010                   optimal   \n",
       " 21             0.0001             0.100                   optimal   \n",
       " 23             0.0001             0.100                   optimal   \n",
       " 14             0.0001             0.010                   optimal   \n",
       " 5              0.0001             0.001                   optimal   \n",
       " 31             0.0010             0.001                   optimal   \n",
       " \n",
       "    classifier__loss   mean_f1    std_f1  \n",
       " 4             hinge  0.781892  0.038922  \n",
       " 13            hinge  0.781892  0.038922  \n",
       " 22            hinge  0.781892  0.038922  \n",
       " 3          log_loss  0.763745  0.011008  \n",
       " 12         log_loss  0.763745  0.011008  \n",
       " 21         log_loss  0.763745  0.011008  \n",
       " 23   modified_huber  0.752576  0.032899  \n",
       " 14   modified_huber  0.752576  0.032899  \n",
       " 5    modified_huber  0.752576  0.032899  \n",
       " 31            hinge  0.742883  0.005001  ,\n",
       " {'classifier__alpha': 0.0001,\n",
       "  'classifier__eta0': 0.001,\n",
       "  'classifier__learning_rate': 'optimal',\n",
       "  'classifier__loss': 'hinge'},\n",
       " {'accuracy': 0.9636129348959931,\n",
       "  'precision': 0.8553615960099751,\n",
       "  'recall': 0.7547940899088337,\n",
       "  'f1': 0.801937207748831,\n",
       "  'confusion_matrix': array([[ 2401,   780],\n",
       "         [  406, 29007]])},\n",
       " {'accuracy': 0.9616320687186829,\n",
       "  'precision': 0.8404605263157895,\n",
       "  'recall': 0.749266862170088,\n",
       "  'f1': 0.7922480620155039,\n",
       "  'confusion_matrix': array([[ 1022,   342],\n",
       "         [  194, 12412]])})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = data[tx_features + agg_features]\n",
    "\n",
    "# Split (sin escalar afuera)\n",
    "X_train_all, X_test_all, y_train, y_test = train_test_split(\n",
    "    X_all, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ========= ColumnTransformer =========\n",
    "numeric_features = X_all.columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features)\n",
    "])\n",
    "\n",
    "# ========= Pipeline base =========\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# ========= Hiperparámetros =========\n",
    "param_grid = {\n",
    "    'classifier__loss': ['log_loss', 'hinge', 'modified_huber'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "    'classifier__eta0': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# ========= GridSearchCV =========\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_all, y_train)\n",
    "\n",
    "# Resultados: f1 promedio y varianza\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "\n",
    "# Resultados\n",
    "results_table = pd.DataFrame(params)\n",
    "results_table['mean_f1'] = means\n",
    "results_table['std_f1'] = stds\n",
    "\n",
    "# Mejor configuración: entrenando al mejor modelo\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Evaluación\n",
    "y_train_pred = best_estimator.predict(X_train_all)\n",
    "y_test_pred = best_estimator.predict(X_test_all)\n",
    "\n",
    "\n",
    "results_train = metrics(y_train, y_train_pred)\n",
    "results_test = metrics(y_test, y_test_pred)\n",
    "\n",
    "results_table.sort_values(by='mean_f1', ascending=False).head(10), best_params, results_train, results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ee2e7280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__alpha': 0.0001, 'classifier__eta0': 0.001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "# imprimir hiperparámetros del mejor modelo\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ce0871c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluación en conjunto de TEST (modelo final)\n",
      "Accuracy: 0.9616320687186829\n",
      "Precision: 0.8404605263157895\n",
      "Recall: 0.749266862170088\n",
      "F1 Score: 0.7922480620155039\n",
      "Matriz de confusión:\n",
      " [[ 1022   342]\n",
      " [  194 12412]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo entrenado\n",
    "print(\"\\n📊 Evaluación en conjunto de TEST (modelo final)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2f9d2",
   "metadata": {},
   "source": [
    "### Interpretación \n",
    "El mejor modelo encontrado es un clasificador tipo SVM lineal entrenado con SGD, con baja regularización (alpha = 0.0001) y un learning rate adaptativo.\n",
    "\n",
    "Se obtiene un F1 score de 0.79, indicando un buen equilibrio entre recall y precisión. \n",
    "\n",
    "El modelo tiene un recall de 0.75, indicando que se detectan tres cuartas partes de los casos de fraude. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3edb0af",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71ece7",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "\n",
    "1. M. Weber, G. Domeniconi, J. Chen, D. K. I. Weidele, C. Bellei, T. Robinson, C. E. Leiserson, \"Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics\", KDD ’19 Workshop on Anomaly Detection in Finance, August 2019, Anchorage, AK, USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
