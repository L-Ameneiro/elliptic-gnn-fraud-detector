{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea55c5f8",
   "metadata": {},
   "source": [
    "## **Practico 3: Entrenamos graph neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3d24e",
   "metadata": {},
   "source": [
    "Se entrenarán tres modelos de GNN:\n",
    "1. GCN\n",
    "2. GraphSAGE\n",
    "3. GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a355137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 1: Imports y configuración básica\n",
    "# ------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfa6b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[203769, 165], edge_index=[2, 234355], y=[203769], train_mask=[203769], test_mask=[203769])\n"
     ]
    }
   ],
   "source": [
    "# CELDA 2: Cargar dataset (PyG)\n",
    "# ------------------------------------------------------------\n",
    "root = 'data/elliptic'   # carpeta donde se guardará/descargará\n",
    "dataset = EllipticBitcoinDataset(root=root)\n",
    "\n",
    "# El dataset entregará un objeto Data (gráfico completo) en dataset[0]\n",
    "data = dataset[0]\n",
    "print(data)\n",
    "# típicamente: Data(x=[N, 166], edge_index=[2, E], y=[N], ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "537e714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int64(0): np.int64(42019), np.int64(1): np.int64(4545), np.int64(2): np.int64(157205)}\n",
      "N nodos: 203769\n",
      "N edges: 234355\n",
      "dim features: 165\n"
     ]
    }
   ],
   "source": [
    "# CELDA 3: Exploración rápida de etiquetas / clases\n",
    "# ------------------------------------------------------------\n",
    "# Observa distribución de etiquetas (en la mayoría de implementaciones: 0 unknown, 1 illicit, 2 licit)\n",
    "ys = data.y.cpu().numpy()\n",
    "(unique, counts) = np.unique(ys, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "# Verifica número de nodos, features:\n",
    "print(\"N nodos:\", data.num_nodes)\n",
    "print(\"N edges:\", data.num_edges)\n",
    "print(\"dim features:\", data.num_node_features if hasattr(data, 'num_node_features') else data.x.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbb691",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_indices = torch.where(data.y >= 0)[0]  # todos los nodos con label válido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f8296ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 4: Pipeline de preprocesamiento (sklearn-style) para tensores\n",
    "# ------------------------------------------------------------\n",
    "# Vamos a construir un Transformer compatible con sklearn que acepta y devuelve\n",
    "# tensores de torch (o arrays) para integrarlo en un Pipeline.\n",
    "class TorchFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encapsula transformaciones sklearn (StandardScaler) para features de nodos\n",
    "    representadas como torch.Tensor.\n",
    "    El pipeline internamente convierte a numpy, aplica scaler y vuelve a torch.\n",
    "    \"\"\"\n",
    "    def __init__(self, scaler=None):\n",
    "        self.scaler = scaler if scaler is not None else StandardScaler()\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # X puede ser torch.Tensor o np.array de shape (N, F)\n",
    "        arr = X.detach().cpu().numpy() if torch.is_tensor(X) else np.asarray(X)\n",
    "        self.scaler.fit(arr)\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        arr = X.detach().cpu().numpy() if torch.is_tensor(X) else np.asarray(X)\n",
    "        arr_t = self.scaler.transform(arr)\n",
    "        return torch.tensor(arr_t, dtype=torch.float32)\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "\n",
    "# Crea pipeline (útil si más pasos se agregan)\n",
    "feature_pipeline = Pipeline([\n",
    "    ('torch_transform', TorchFeatureTransformer(scaler=StandardScaler()))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7e97c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train: 129400 N val: 16175 N test: 16175\n"
     ]
    }
   ],
   "source": [
    "# CELDA 5: Preparar X, y y máscaras de entrenamiento/validación/test\n",
    "# ------------------------------------------------------------\n",
    "# Etiquetas: 0 = unknown, 1 = illicit, 2 = licit  (verifica en tu versión si cambia)\n",
    "# Nosotros definimos problema binario: illicit (1) vs licit (0). Filtramos 'unknown'.\n",
    "labels = data.y.clone().cpu()  # tensor (N,)\n",
    "# Crea máscara de nodos etiquetados (no-unknown)\n",
    "labeled_mask = labels != 0\n",
    "\n",
    "# Mapear: illicit -> 1, licit -> 0\n",
    "# asumimos: 1 = illicit, 2 = licit -> transformar a {0,1}\n",
    "y_binary = labels.clone()\n",
    "y_binary[labels == 2] = 0  # licit -> 0\n",
    "y_binary[labels == 1] = 1  # illicit -> 1\n",
    "y_binary[labels == 0] = -1  # unknown marcado como -1 para ignorar\n",
    "\n",
    "# Extraer índices etiquetados\n",
    "labeled_idx = torch.where(labeled_mask)[0].cpu().numpy()\n",
    "y_labeled = y_binary[labeled_idx].cpu().numpy().astype(int)\n",
    "\n",
    "# Hacemos un split estratificado sobre los nodos etiquetados\n",
    "train_idx_rel, test_idx_rel, y_train_rel, y_test_rel = train_test_split(\n",
    "    np.arange(len(labeled_idx)),\n",
    "    y_labeled,\n",
    "    test_size=0.2,\n",
    "    stratify=y_labeled,\n",
    "    random_state=42\n",
    ")\n",
    "train_idx = labeled_idx[train_idx_rel]\n",
    "test_idx = labeled_idx[test_idx_rel]\n",
    "\n",
    "# De test separamos validación (80/20 sobre test)\n",
    "val_rel, test_rel, y_val_rel, y_test_rel = train_test_split(\n",
    "    np.arange(len(test_idx)),\n",
    "    y_labeled[test_idx_rel],\n",
    "    test_size=0.5,\n",
    "    stratify=y_labeled[test_idx_rel],\n",
    "    random_state=42\n",
    ")\n",
    "val_idx = test_idx[val_rel]\n",
    "test_idx = test_idx[test_rel]\n",
    "\n",
    "# Construimos boolean masks en formato torch (útiles para PyG training)\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "val_mask   = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask  = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "print(\"N train:\", train_mask.sum().item(), \"N val:\", val_mask.sum().item(), \"N test:\", test_mask.sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1890aba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features transformadas. shape: torch.Size([203769, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lunix/Documents/DiploDatos/Mentoria/elliptic-gnn-fraud-detector/.venv/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# CELDA 6: Aplicar pipeline de features (fit sólo con nodos de entrenamiento o con todos?)\n",
    "# ------------------------------------------------------------\n",
    "# Opción 1 (recomendado): ajustar scaler usando sólo nodos de entrenamiento etiquetados\n",
    "X = data.x  # tensor [N, F]\n",
    "feature_pipeline.fit(X[train_mask])   # ajustamos con distrib. de train\n",
    "X_trans = feature_pipeline.transform(X)  # tensor transformado (N, F)\n",
    "data.x = X_trans  # reemplazamos features en el objeto Data\n",
    "print(\"Features transformadas. shape:\", data.x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7084ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 7: Definir modelo GCN simple (2 capas)\n",
    "# ------------------------------------------------------------\n",
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, out_channels=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x  # logits (N, out_channels)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCNNet(in_channels=data.num_node_features, hidden_channels=64, out_channels=2, dropout=0.5).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "730325ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 2.0239 | Train F1: 0.0458 | Val F1: 0.0455\n",
      "Epoch 010 | Loss: 0.2134 | Train F1: 0.0000 | Val F1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lunix/Documents/DiploDatos/Mentoria/elliptic-gnn-fraud-detector/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/lunix/Documents/DiploDatos/Mentoria/elliptic-gnn-fraud-detector/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/lunix/Documents/DiploDatos/Mentoria/elliptic-gnn-fraud-detector/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/lunix/Documents/DiploDatos/Mentoria/elliptic-gnn-fraud-detector/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/lunix/Documents/DiploDatos/Mentoria/elliptic-gnn-fraud-detector/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/lunix/Documents/DiploDatos/Mentoria/elliptic-gnn-fraud-detector/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 | Loss: 0.1834 | Train F1: 0.0277 | Val F1: 0.0336\n",
      "Epoch 030 | Loss: 0.1460 | Train F1: 0.0347 | Val F1: 0.0300\n",
      "Epoch 040 | Loss: 0.1297 | Train F1: 0.0256 | Val F1: 0.0298\n",
      "Epoch 050 | Loss: 0.1158 | Train F1: 0.0793 | Val F1: 0.0677\n",
      "Epoch 060 | Loss: 0.1118 | Train F1: 0.1031 | Val F1: 0.0882\n",
      "Epoch 070 | Loss: 0.1052 | Train F1: 0.0983 | Val F1: 0.0842\n",
      "Epoch 080 | Loss: 0.1036 | Train F1: 0.1067 | Val F1: 0.0882\n",
      "Epoch 090 | Loss: 0.1012 | Train F1: 0.1077 | Val F1: 0.1002\n",
      "Epoch 100 | Loss: 0.0997 | Train F1: 0.1131 | Val F1: 0.1042\n",
      "Epoch 110 | Loss: 0.0980 | Train F1: 0.1087 | Val F1: 0.1042\n",
      "Epoch 120 | Loss: 0.0974 | Train F1: 0.1174 | Val F1: 0.1159\n",
      "Epoch 130 | Loss: 0.0962 | Train F1: 0.1194 | Val F1: 0.1237\n",
      "Epoch 140 | Loss: 0.0949 | Train F1: 0.1179 | Val F1: 0.1276\n",
      "Epoch 150 | Loss: 0.0947 | Train F1: 0.1237 | Val F1: 0.1276\n",
      "Epoch 160 | Loss: 0.0940 | Train F1: 0.1223 | Val F1: 0.1314\n",
      "Epoch 170 | Loss: 0.0936 | Train F1: 0.1324 | Val F1: 0.1391\n",
      "Epoch 180 | Loss: 0.0926 | Train F1: 0.1251 | Val F1: 0.1237\n",
      "Epoch 190 | Loss: 0.0918 | Train F1: 0.1352 | Val F1: 0.1388\n",
      "Epoch 200 | Loss: 0.0916 | Train F1: 0.1347 | Val F1: 0.1385\n"
     ]
    }
   ],
   "source": [
    "# CELDA 8: Training loop (usando masks)\n",
    "# ------------------------------------------------------------\n",
    "def train(model, data, train_mask, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)  # logits for all nodes (N, 2)\n",
    "    # aplicamos mask para loss (solo nodos de entrenamiento)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask].long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    probs = F.softmax(out, dim=1)\n",
    "    preds = probs.argmax(dim=1)\n",
    "    y_true = data.y[mask].cpu().numpy()\n",
    "    y_pred = preds[mask].cpu().numpy()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', pos_label=1)\n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'preds': preds}\n",
    "\n",
    "# Ajuste: las etiquetas de data.y deben ser 0 o 1 para nodo etiquetado.\n",
    "# En nuestro pipeline, transformamos labels (2->0, 1->1). Asegurémonos:\n",
    "# los nodos unknown deben quedar fuera por las masks (no se usarán).\n",
    "data.y = y_binary.to(device)  # contiene -1 para unknown; convertimos a device\n",
    "\n",
    "# Entrenamiento\n",
    "n_epochs = 200\n",
    "best_val_f1 = 0.0\n",
    "best_state = None\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(model, data, train_mask.to(device), optimizer)\n",
    "    eval_train = evaluate(model, data, train_mask.to(device))\n",
    "    eval_val = evaluate(model, data, val_mask.to(device))\n",
    "    if eval_val['f1'] > best_val_f1:\n",
    "        best_val_f1 = eval_val['f1']\n",
    "        best_state = model.state_dict()\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Train F1: {eval_train['f1']:.4f} | Val F1: {eval_val['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1018cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results:\n",
      "Accuracy: 0.9740\n",
      "Precision: 1.0000\n",
      "Recall: 0.0749\n",
      "F1: 0.1393\n"
     ]
    }
   ],
   "source": [
    "# CELDA 9: Evaluación final sobre test set (usar mejor modelo por val)\n",
    "# ------------------------------------------------------------\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "eval_test = evaluate(model, data, test_mask.to(device))\n",
    "print(\"Test results:\")\n",
    "print(f\"Accuracy: {eval_test['accuracy']:.4f}\")\n",
    "print(f\"Precision: {eval_test['precision']:.4f}\")\n",
    "print(f\"Recall: {eval_test['recall']:.4f}\")\n",
    "print(f\"F1: {eval_test['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcdcadcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando in-degree y out-degree centrality...\n",
      "Calculando betweenness centrality dirigido (aprox)...\n",
      "Calculando closeness centrality dirigido...\n",
      "Nuevo shape de data.x con centralidades dirigidas: torch.Size([203769, 169])\n"
     ]
    }
   ],
   "source": [
    "# CELDA EXTRA: Centralidades considerando grafo dirigido\n",
    "# -------------------------------------------------------\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Convertimos a grafo dirigido\n",
    "G_dir = to_networkx(data, to_undirected=False)\n",
    "\n",
    "# In-degree y Out-degree centrality\n",
    "print(\"Calculando in-degree y out-degree centrality...\")\n",
    "in_deg_cent = nx.in_degree_centrality(G_dir)\n",
    "out_deg_cent = nx.out_degree_centrality(G_dir)\n",
    "\n",
    "# Betweenness centrality dirigido (aproximado para acelerar)\n",
    "print(\"Calculando betweenness centrality dirigido (aprox)...\")\n",
    "# Betweenness centrality aproximada en grafo dirigido\n",
    "btw_cent = nx.betweenness_centrality(G_dir, k=500, seed=42, normalized=True)\n",
    "\n",
    "# Closeness centrality dirigido\n",
    "print(\"Calculando closeness centrality dirigido...\")\n",
    "clo_cent = nx.closeness_centrality(G_dir)\n",
    "\n",
    "# Eigenvector centrality dirigido (puede tardar)\n",
    "#print(\"Calculando eigenvector centrality dirigido...\")\n",
    "#eig_cent = nx.eigenvector_centrality_numpy(G_dir, max_iter=500)\n",
    "\n",
    "# Convertimos a tensores en orden de índices de nodos\n",
    "n_nodes = data.num_nodes\n",
    "in_deg_feat = torch.tensor([in_deg_cent[i] for i in range(n_nodes)], dtype=torch.float32).view(-1,1)\n",
    "out_deg_feat = torch.tensor([out_deg_cent[i] for i in range(n_nodes)], dtype=torch.float32).view(-1,1)\n",
    "btw_feat = torch.tensor([btw_cent[i] for i in range(n_nodes)], dtype=torch.float32).view(-1,1)\n",
    "clo_feat = torch.tensor([clo_cent[i] for i in range(n_nodes)], dtype=torch.float32).view(-1,1)\n",
    "#eig_feat = torch.tensor([eig_cent[i] for i in range(n_nodes)], dtype=torch.float32).view(-1,1)\n",
    "\n",
    "# Concatenamos todas las centralidades\n",
    "centrality_feats = torch.cat([in_deg_feat, out_deg_feat, btw_feat, clo_feat], dim=1)\n",
    "\n",
    "# Normalizamos centralidades usando pipeline ya definido\n",
    "centrality_feats = feature_pipeline.fit_transform(centrality_feats)\n",
    "\n",
    "# Concatenamos con features originales\n",
    "data.x = torch.cat([data.x, centrality_feats], dim=1)\n",
    "print(\"Nuevo shape de data.x con centralidades dirigidas:\", data.x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dca8c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features transformadas. shape: torch.Size([203769, 169])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lunix/Documents/DiploDatos/Mentoria/elliptic-gnn-fraud-detector/.venv/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# CELDA 6: Aplicar pipeline de features (fit sólo con nodos de entrenamiento o con todos?)\n",
    "# ------------------------------------------------------------\n",
    "# Opción 1 (recomendado): ajustar scaler usando sólo nodos de entrenamiento etiquetados\n",
    "X = data.x  # tensor [N, F]\n",
    "feature_pipeline.fit(X[train_mask])   # ajustamos con distrib. de train\n",
    "X_trans = feature_pipeline.transform(X)  # tensor transformado (N, F)\n",
    "data.x = X_trans  # reemplazamos features en el objeto Data\n",
    "print(\"Features transformadas. shape:\", data.x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1483b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 7: Definir modelo GCN simple (2 capas)\n",
    "# ------------------------------------------------------------\n",
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, out_channels=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x  # logits (N, out_channels)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCNNet(in_channels=data.num_node_features, hidden_channels=64, out_channels=2, dropout=0.5).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2aa3554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.8135 | Train F1: 0.0203 | Val F1: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lunix/Documents/DiploDatos/Mentoria/elliptic-gnn-fraud-detector/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 | Loss: 0.1802 | Train F1: 0.0016 | Val F1: 0.0087\n",
      "Epoch 020 | Loss: 0.1327 | Train F1: 0.0707 | Val F1: 0.0669\n",
      "Epoch 030 | Loss: 0.1135 | Train F1: 0.0895 | Val F1: 0.0996\n",
      "Epoch 040 | Loss: 0.1052 | Train F1: 0.0731 | Val F1: 0.0720\n",
      "Epoch 050 | Loss: 0.1003 | Train F1: 0.1092 | Val F1: 0.1198\n",
      "Epoch 060 | Loss: 0.0982 | Train F1: 0.0823 | Val F1: 0.0720\n",
      "Epoch 070 | Loss: 0.0965 | Train F1: 0.0958 | Val F1: 0.0882\n",
      "Epoch 080 | Loss: 0.0953 | Train F1: 0.0893 | Val F1: 0.0842\n",
      "Epoch 090 | Loss: 0.0935 | Train F1: 0.1052 | Val F1: 0.1042\n",
      "Epoch 100 | Loss: 0.0925 | Train F1: 0.1042 | Val F1: 0.1042\n",
      "Epoch 110 | Loss: 0.0920 | Train F1: 0.1037 | Val F1: 0.1042\n",
      "Epoch 120 | Loss: 0.0912 | Train F1: 0.1052 | Val F1: 0.1118\n",
      "Epoch 130 | Loss: 0.0908 | Train F1: 0.1101 | Val F1: 0.1079\n",
      "Epoch 140 | Loss: 0.0895 | Train F1: 0.1314 | Val F1: 0.1311\n",
      "Epoch 150 | Loss: 0.0890 | Train F1: 0.1252 | Val F1: 0.1273\n",
      "Epoch 160 | Loss: 0.0884 | Train F1: 0.1271 | Val F1: 0.1347\n",
      "Epoch 170 | Loss: 0.0874 | Train F1: 0.1299 | Val F1: 0.1309\n",
      "Epoch 180 | Loss: 0.0871 | Train F1: 0.1357 | Val F1: 0.1460\n",
      "Epoch 190 | Loss: 0.0867 | Train F1: 0.1347 | Val F1: 0.1423\n",
      "Epoch 200 | Loss: 0.0869 | Train F1: 0.1361 | Val F1: 0.1270\n"
     ]
    }
   ],
   "source": [
    "# CELDA 8: Training loop (usando masks)\n",
    "# ------------------------------------------------------------\n",
    "def train(model, data, train_mask, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)  # logits for all nodes (N, 2)\n",
    "    # aplicamos mask para loss (solo nodos de entrenamiento)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask].long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    probs = F.softmax(out, dim=1)\n",
    "    preds = probs.argmax(dim=1)\n",
    "    y_true = data.y[mask].cpu().numpy()\n",
    "    y_pred = preds[mask].cpu().numpy()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', pos_label=1)\n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'preds': preds}\n",
    "\n",
    "# Ajuste: las etiquetas de data.y deben ser 0 o 1 para nodo etiquetado.\n",
    "# En nuestro pipeline, transformamos labels (2->0, 1->1). Asegurémonos:\n",
    "# los nodos unknown deben quedar fuera por las masks (no se usarán).\n",
    "data.y = y_binary.to(device)  # contiene -1 para unknown; convertimos a device\n",
    "\n",
    "# Entrenamiento\n",
    "n_epochs = 200\n",
    "best_val_f1 = 0.0\n",
    "best_state = None\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(model, data, train_mask.to(device), optimizer)\n",
    "    eval_train = evaluate(model, data, train_mask.to(device))\n",
    "    eval_val = evaluate(model, data, val_mask.to(device))\n",
    "    if eval_val['f1'] > best_val_f1:\n",
    "        best_val_f1 = eval_val['f1']\n",
    "        best_state = model.state_dict()\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Train F1: {eval_train['f1']:.4f} | Val F1: {eval_val['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b07d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results:\n",
      "Accuracy: 0.9743\n",
      "Precision: 1.0000\n",
      "Recall: 0.0859\n",
      "F1: 0.1582\n"
     ]
    }
   ],
   "source": [
    "# CELDA 9: Evaluación final sobre test set (usar mejor modelo por val)\n",
    "# ------------------------------------------------------------\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "eval_test = evaluate(model, data, test_mask.to(device))\n",
    "print(\"Test results:\")\n",
    "print(f\"Accuracy: {eval_test['accuracy']:.4f}\")\n",
    "print(f\"Precision: {eval_test['precision']:.4f}\")\n",
    "print(f\"Recall: {eval_test['recall']:.4f}\")\n",
    "print(f\"F1: {eval_test['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441d3f5",
   "metadata": {},
   "source": [
    "## Balanceo de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02fe1c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int64(0): np.int64(125764), np.int64(1): np.int64(3636)}\n",
      "Pesos de clase: tensor([ 1.0000, 34.5886])\n"
     ]
    }
   ],
   "source": [
    "# Calcular distribución de clases en nodos de entrenamiento\n",
    "y_train = data.y[train_mask].cpu().numpy()\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(classes, counts)))\n",
    "\n",
    "# Clases: 0 = licit, 1 = illicit\n",
    "# Peso inversamente proporcional a frecuencia\n",
    "weight_licit = 1.0\n",
    "weight_illicit = counts[0] / counts[1]  # mayor peso para la clase minoritaria\n",
    "\n",
    "class_weights = torch.tensor([weight_licit, weight_illicit], dtype=torch.float32).to(device)\n",
    "print(\"Pesos de clase:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1853c55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features transformadas. shape: torch.Size([203769, 169])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lunix/Documents/DiploDatos/Mentoria/elliptic-gnn-fraud-detector/.venv/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Aplicar pipeline de features sobre las features nuevas (originales + centralidades)\n",
    "X = data.x\n",
    "feature_pipeline.fit(X[train_mask])   # ajustamos con nodos de entrenamiento\n",
    "X_trans = feature_pipeline.transform(X)\n",
    "data.x = X_trans\n",
    "print(\"Features transformadas. shape:\", data.x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60a5ab",
   "metadata": {},
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCNNet(\n",
    "    in_channels=data.num_node_features,  # actualizar in_channels\n",
    "    hidden_channels=64,\n",
    "    out_channels=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "data = data.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "494ffe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.0865 | Train F1: 0.1323 | Val F1: 0.1309\n",
      "Epoch 010 | Loss: 0.0864 | Train F1: 0.1497 | Val F1: 0.1535\n",
      "Epoch 020 | Loss: 0.0857 | Train F1: 0.1393 | Val F1: 0.1385\n",
      "Epoch 030 | Loss: 0.0857 | Train F1: 0.1497 | Val F1: 0.1501\n",
      "Epoch 040 | Loss: 0.0857 | Train F1: 0.1585 | Val F1: 0.1498\n",
      "Epoch 050 | Loss: 0.0850 | Train F1: 0.1530 | Val F1: 0.1347\n",
      "Epoch 060 | Loss: 0.0848 | Train F1: 0.1650 | Val F1: 0.1498\n",
      "Epoch 070 | Loss: 0.0846 | Train F1: 0.1607 | Val F1: 0.1535\n",
      "Epoch 080 | Loss: 0.0843 | Train F1: 0.1728 | Val F1: 0.1501\n",
      "Epoch 090 | Loss: 0.0840 | Train F1: 0.1816 | Val F1: 0.1687\n",
      "Epoch 100 | Loss: 0.0839 | Train F1: 0.1705 | Val F1: 0.1650\n",
      "Epoch 110 | Loss: 0.0836 | Train F1: 0.1696 | Val F1: 0.1613\n",
      "Epoch 120 | Loss: 0.0833 | Train F1: 0.1786 | Val F1: 0.1720\n",
      "Epoch 130 | Loss: 0.0837 | Train F1: 0.1826 | Val F1: 0.1793\n",
      "Epoch 140 | Loss: 0.0831 | Train F1: 0.1796 | Val F1: 0.1793\n",
      "Epoch 150 | Loss: 0.0832 | Train F1: 0.1718 | Val F1: 0.1756\n",
      "Epoch 160 | Loss: 0.0826 | Train F1: 0.1826 | Val F1: 0.1793\n",
      "Epoch 170 | Loss: 0.0823 | Train F1: 0.1847 | Val F1: 0.1865\n",
      "Epoch 180 | Loss: 0.0820 | Train F1: 0.1789 | Val F1: 0.1829\n",
      "Epoch 190 | Loss: 0.0825 | Train F1: 0.1837 | Val F1: 0.1865\n",
      "Epoch 200 | Loss: 0.0819 | Train F1: 0.1775 | Val F1: 0.1793\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "best_val_f1 = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask].long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    eval_train = evaluate(model, data, train_mask)\n",
    "    eval_val = evaluate(model, data, val_mask)\n",
    "    \n",
    "    if eval_val['f1'] > best_val_f1:\n",
    "        best_val_f1 = eval_val['f1']\n",
    "        best_state = model.state_dict()\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Train F1: {eval_train['f1']:.4f} | Val F1: {eval_val['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ec78b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results con centralidades dirigidas:\n",
      "Accuracy: 0.9742\n",
      "Precision: 0.9091\n",
      "Recall: 0.0881\n",
      "F1: 0.1606\n"
     ]
    }
   ],
   "source": [
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "eval_test = evaluate(model, data, test_mask)\n",
    "print(\"Test results con centralidades dirigidas:\")\n",
    "print(f\"Accuracy: {eval_test['accuracy']:.4f}\")\n",
    "print(f\"Precision: {eval_test['precision']:.4f}\")\n",
    "print(f\"Recall: {eval_test['recall']:.4f}\")\n",
    "print(f\"F1: {eval_test['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9dc37",
   "metadata": {},
   "source": [
    "Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1b7fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "318f690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesos para la clase minoritaria\n",
    "# Alpha suavizado\n",
    "alpha = torch.tensor([1.0, 5.0], dtype=torch.float32).to(device)  # menor que antes (~30-40)\n",
    "criterion = FocalLoss(alpha=alpha, gamma=1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e701df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.2202 | Train F1: 0.3354 | Val F1: 0.3045\n",
      "Epoch 010 | Loss: 0.1418 | Train F1: 0.4231 | Val F1: 0.3721\n",
      "Epoch 020 | Loss: 0.1318 | Train F1: 0.4524 | Val F1: 0.3997\n",
      "Epoch 030 | Loss: 0.1252 | Train F1: 0.4689 | Val F1: 0.4081\n",
      "Epoch 040 | Loss: 0.1229 | Train F1: 0.4846 | Val F1: 0.4265\n",
      "Epoch 050 | Loss: 0.1215 | Train F1: 0.5029 | Val F1: 0.4390\n",
      "Epoch 060 | Loss: 0.1204 | Train F1: 0.5028 | Val F1: 0.4436\n",
      "Epoch 070 | Loss: 0.1197 | Train F1: 0.5059 | Val F1: 0.4502\n",
      "Epoch 080 | Loss: 0.1196 | Train F1: 0.5101 | Val F1: 0.4451\n",
      "Epoch 090 | Loss: 0.1192 | Train F1: 0.5114 | Val F1: 0.4571\n",
      "Epoch 100 | Loss: 0.1188 | Train F1: 0.5149 | Val F1: 0.4576\n",
      "Epoch 110 | Loss: 0.1180 | Train F1: 0.5209 | Val F1: 0.4601\n",
      "Epoch 120 | Loss: 0.1182 | Train F1: 0.5194 | Val F1: 0.4611\n",
      "Epoch 130 | Loss: 0.1175 | Train F1: 0.5245 | Val F1: 0.4554\n",
      "Epoch 140 | Loss: 0.1170 | Train F1: 0.5254 | Val F1: 0.4578\n",
      "Epoch 150 | Loss: 0.1166 | Train F1: 0.5282 | Val F1: 0.4624\n",
      "Epoch 160 | Loss: 0.1166 | Train F1: 0.5289 | Val F1: 0.4647\n",
      "Epoch 170 | Loss: 0.1172 | Train F1: 0.5325 | Val F1: 0.4741\n",
      "Epoch 180 | Loss: 0.1160 | Train F1: 0.5294 | Val F1: 0.4669\n",
      "Epoch 190 | Loss: 0.1171 | Train F1: 0.5281 | Val F1: 0.4643\n",
      "Epoch 200 | Loss: 0.1161 | Train F1: 0.5317 | Val F1: 0.4651\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "best_val_f1 = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask].long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    eval_train = evaluate(model, data, train_mask)\n",
    "    eval_val = evaluate(model, data, val_mask)\n",
    "    \n",
    "    if eval_val['f1'] > best_val_f1:\n",
    "        best_val_f1 = eval_val['f1']\n",
    "        best_state = model.state_dict()\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Train F1: {eval_train['f1']:.4f} | Val F1: {eval_val['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0f4d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results con focal loss:\n",
      "Accuracy: 0.8161\n",
      "Precision: 0.1250\n",
      "Recall: 0.9251\n",
      "F1: 0.2202\n"
     ]
    }
   ],
   "source": [
    "eval_test = evaluate(model, data, test_mask)\n",
    "print(\"Test results con focal loss:\")\n",
    "print(f\"Accuracy: {eval_test['accuracy']:.4f}\")\n",
    "print(f\"Precision: {eval_test['precision']:.4f}\")\n",
    "print(f\"Recall: {eval_test['recall']:.4f}\")\n",
    "print(f\"F1: {eval_test['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e6519",
   "metadata": {},
   "source": [
    "## Random shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "affe76ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train: 97050 N val: 32350 N test: 32350\n",
      "Epoch 001 | Loss: 1.7423 | Train F1: 0.0729 | Val F1: 0.0717\n",
      "Epoch 010 | Loss: 0.7867 | Train F1: 0.1387 | Val F1: 0.1407\n",
      "Epoch 020 | Loss: 0.6549 | Train F1: 0.1419 | Val F1: 0.1439\n",
      "Epoch 030 | Loss: 0.5988 | Train F1: 0.1641 | Val F1: 0.1664\n",
      "Epoch 040 | Loss: 0.5465 | Train F1: 0.1802 | Val F1: 0.1800\n",
      "Epoch 050 | Loss: 0.5262 | Train F1: 0.1855 | Val F1: 0.1840\n",
      "Epoch 060 | Loss: 0.5055 | Train F1: 0.1973 | Val F1: 0.1927\n",
      "Epoch 070 | Loss: 0.4857 | Train F1: 0.2056 | Val F1: 0.2007\n",
      "Epoch 080 | Loss: 0.4678 | Train F1: 0.2203 | Val F1: 0.2155\n",
      "Epoch 090 | Loss: 0.4585 | Train F1: 0.2224 | Val F1: 0.2165\n",
      "Epoch 100 | Loss: 0.4496 | Train F1: 0.2315 | Val F1: 0.2268\n",
      "Epoch 110 | Loss: 0.4372 | Train F1: 0.2420 | Val F1: 0.2320\n",
      "Epoch 120 | Loss: 0.4282 | Train F1: 0.2517 | Val F1: 0.2390\n",
      "Epoch 130 | Loss: 0.4178 | Train F1: 0.2540 | Val F1: 0.2417\n",
      "Epoch 140 | Loss: 0.4198 | Train F1: 0.2663 | Val F1: 0.2538\n",
      "Epoch 150 | Loss: 0.4111 | Train F1: 0.2595 | Val F1: 0.2453\n",
      "Epoch 160 | Loss: 0.4008 | Train F1: 0.2696 | Val F1: 0.2536\n",
      "Epoch 170 | Loss: 0.3929 | Train F1: 0.2644 | Val F1: 0.2516\n",
      "Epoch 180 | Loss: 0.3935 | Train F1: 0.2799 | Val F1: 0.2636\n",
      "Epoch 190 | Loss: 0.3819 | Train F1: 0.2867 | Val F1: 0.2726\n",
      "Epoch 200 | Loss: 0.3867 | Train F1: 0.2882 | Val F1: 0.2747\n",
      "Test results:\n",
      "Accuracy: 0.8628\n",
      "Precision: 0.1606\n",
      "Recall: 0.8906\n",
      "F1: 0.2721\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# CELDA COMPLETA: Procesamiento, split, modelo, entrenamiento y evaluación\n",
    "# ------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Preparar labels y nodos etiquetados\n",
    "# -------------------------\n",
    "# Mapear etiquetas a binario: illicit=1, licit=0, unknown=-1\n",
    "labels = data.y.clone().cpu()\n",
    "y_binary = labels.clone()\n",
    "\n",
    "\n",
    "# Nodos etiquetados (no unknown)\n",
    "labeled_indices = torch.where(y_binary >= 0)[0]\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ Split 60/20/20 con shuffle reproducible\n",
    "# -------------------------\n",
    "num_labeled = labeled_indices.shape[0]\n",
    "num_train = int(0.6 * num_labeled)\n",
    "num_val   = int(0.2 * num_labeled)\n",
    "num_test  = num_labeled - num_train - num_val\n",
    "\n",
    "torch.manual_seed(42)\n",
    "perm = torch.randperm(num_labeled)\n",
    "\n",
    "train_idx = labeled_indices[perm[:num_train]]\n",
    "val_idx   = labeled_indices[perm[num_train:num_train + num_val]]\n",
    "test_idx  = labeled_indices[perm[num_train + num_val:]]\n",
    "\n",
    "# Construir máscaras booleanas\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "val_mask   = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask  = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "# Asignar al objeto data\n",
    "data.train_mask = train_mask\n",
    "data.val_mask   = val_mask\n",
    "data.test_mask  = test_mask\n",
    "data.y_binary   = y_binary\n",
    "\n",
    "print(\"N train:\", train_mask.sum().item(),\n",
    "      \"N val:\", val_mask.sum().item(),\n",
    "      \"N test:\", test_mask.sum().item())\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Definir modelo GCN\n",
    "# -------------------------\n",
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Definir Focal Loss opcional\n",
    "# -------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# -------------------------\n",
    "# 5️⃣ Inicializar modelo y optimizer\n",
    "# -------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "in_channels = data.x.shape[1]\n",
    "hidden_channels = 64\n",
    "out_channels = 2\n",
    "model = GCNNet(in_channels, hidden_channels, out_channels).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Pesos de clase (opcional para Focal Loss)\n",
    "class_counts = torch.bincount(data.y_binary[train_mask])\n",
    "class_weights = (class_counts.sum() / class_counts).float().to(device)\n",
    "criterion = FocalLoss(alpha=class_weights, gamma=1.5)  # gamma ajustable\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# -------------------------\n",
    "# 6️⃣ Función de evaluación\n",
    "# -------------------------\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data)\n",
    "        preds = logits[mask].argmax(dim=1).cpu().numpy()\n",
    "        labels_true = data.y_binary[mask].cpu().numpy()\n",
    "        acc = accuracy_score(labels_true, preds)\n",
    "        prec = precision_score(labels_true, preds, zero_division=0)\n",
    "        rec = recall_score(labels_true, preds, zero_division=0)\n",
    "        f1 = f1_score(labels_true, preds, zero_division=0)\n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
    "\n",
    "# -------------------------\n",
    "# 7️⃣ Loop de entrenamiento\n",
    "# -------------------------\n",
    "n_epochs = 200\n",
    "best_val_f1 = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y_binary[train_mask].long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    eval_train = evaluate(model, data, train_mask)\n",
    "    eval_val = evaluate(model, data, val_mask)\n",
    "\n",
    "    if eval_val['f1'] > best_val_f1:\n",
    "        best_val_f1 = eval_val['f1']\n",
    "        best_state = model.state_dict()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | \"\n",
    "              f\"Train F1: {eval_train['f1']:.4f} | Val F1: {eval_val['f1']:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 8️⃣ Evaluación final en test set\n",
    "# -------------------------\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "eval_test = evaluate(model, data, test_mask)\n",
    "print(\"Test results:\")\n",
    "print(f\"Accuracy: {eval_test['accuracy']:.4f}\")\n",
    "print(f\"Precision: {eval_test['precision']:.4f}\")\n",
    "print(f\"Recall: {eval_test['recall']:.4f}\")\n",
    "print(f\"F1: {eval_test['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19b9daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodos en data: 203769\n",
      "Total nodos con label original 1 o 2: 161750\n",
      "Total nodos después de mapear y_binary >=0: 4545\n"
     ]
    }
   ],
   "source": [
    "print(\"Total nodos en data:\", data.num_nodes)\n",
    "print(\"Total nodos con label original 1 o 2:\", ((labels==0) | (labels==1)).sum().item())\n",
    "print(\"Total nodos después de mapear y_binary >=0:\", (y_binary >=0).sum().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
